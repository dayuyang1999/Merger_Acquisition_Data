{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:25:22.294263Z",
     "start_time": "2021-09-11T15:25:22.291267Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime \n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "import wrds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper func\n",
    "\n",
    "\n",
    "[A simple python program to automate the SQL query building for Whartonâ€™s WRDS](https://imagoodboy.com/post/wrds_helper/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:41:28.385760Z",
     "start_time": "2021-09-11T15:41:28.373418Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_query(dataset, **kwargs):\n",
    "    \"\"\"Build Query for WRDS database.\n",
    "        Args:\n",
    "            dataset (str): dataset\n",
    "            start (str): Start of time period with format YYYY-MM-DD, default to None\n",
    "            end (str): End of time period with format YYYY-MM-DD, default to None\n",
    "            columns (str or list): Filter of one or a list of columns, default to None\n",
    "            sic (str or list): Filter of one or a list of SIC, default to None\n",
    "            sich (str or list): Filter of one or a list of historical SIC, default to None\n",
    "            gvkey (str or list):  Filter of one or a list of Global Company Key, default to None\n",
    "            tic (str or list):  Filter of one or a list of ticker symbols, default to None\n",
    "            cusip (str or list):  Filter of one or a list of CUSIP symbols, default to None\n",
    "            limit (int): Return only a number of return records, defualt to None\n",
    "        Returns:\n",
    "            (str) with query\n",
    "    \"\"\"\n",
    "    columns = kwargs.get(\"columns\", None)\n",
    "    sic = kwargs.get(\"sic\", None)\n",
    "    sich = kwargs.get(\"sich\", None)\n",
    "    start = kwargs.get(\"start\", None)\n",
    "    end = kwargs.get(\"end\", None)\n",
    "    gvkey = kwargs.get(\"gvkey\", None)\n",
    "    tic = kwargs.get(\"tic\", None)\n",
    "    cusip = kwargs.get(\"cusip\", None)\n",
    "    limit = kwargs.get(\"limit\", None)\n",
    "\n",
    "\n",
    "    if columns is not None:\n",
    "        columns_filter = \",\".join([x for x in columns])\n",
    "    else:\n",
    "        columns_filter = \"*\"\n",
    "\n",
    "    date_filter = \"\"\n",
    "    if start and end is not None:\n",
    "        date_filter = \"datadate between {} and {}\".format(\"'\" + start + \"'\" , \"'\" + end + \"'\")\n",
    "\n",
    "    sic_filter = \"\"\n",
    "    if sic is not None:\n",
    "        sic_filter = \"sic in ({}) \".format(\n",
    "            \",\".join([\"'\" + x + \"'\" for x in sic])\n",
    "        )\n",
    "\n",
    "    sich_filter = \"\"\n",
    "    if sich is not None:\n",
    "        sich_filter = \"sich in ({}) \".format(\n",
    "            \",\".join([\"'\" + x + \"'\" for x in sich])\n",
    "        )\n",
    "\n",
    "    gvkey_filter = \"\"\n",
    "    if gvkey is not None:\n",
    "        gvkey_filter = \"gvkey in ({}) \".format(\n",
    "            \",\".join([\"'\" + x + \"'\" for x in gvkey])\n",
    "        )\n",
    "\n",
    "    tic_filter = \"\"\n",
    "    if tic is not None:\n",
    "        tic_filter = \"tic in ({}) \".format(\n",
    "            \",\".join([\"'\" + x + \"'\" for x in tic])\n",
    "        )\n",
    "\n",
    "    cusip_filter = \"\"\n",
    "    if cusip is not None:\n",
    "        cusip_filter = \"cusip in ({}) \".format(\n",
    "            \",\".join([\"'\" + x + \"'\" for x in cusip])\n",
    "        )\n",
    "\n",
    "    # FIRST INSTANCE WITHOUT AND THEN SEQUENTIALLY ADD FILTERS (if non empty)\n",
    "    filters = [date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter]\n",
    "    filters_list = \"\"\n",
    "    if any(filters):\n",
    "        filters_list = \" and \".join([x for x in filters if x != \"\"])\n",
    "        filters_list = \"where \" + filters_list\n",
    "\n",
    "    limit_number = \"\"\n",
    "    if limit:\n",
    "        limit_number += \"LIMIT {} \".format(limit)\n",
    "\n",
    "    cmd = (\n",
    "        \"select \"\n",
    "        + columns_filter\n",
    "        + \" from {} \".format(dataset)\n",
    "        + filters_list\n",
    "        + limit_number\n",
    "        )\n",
    "    \n",
    "    #print(cmd)\n",
    "\n",
    "    return(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 WRDS and EVANS Mapping, which one is more trustworthy?\n",
    "\n",
    "Q: if either side of the participants of a MA deal is a private firm (Either target or acquiror or both). How could `Evans_bridge` get the `GVKEY` info?\n",
    "- Does it use the parents' `GVKEY` as the substitution of participants' `GVKEY`?\n",
    "\n",
    "\n",
    "for anwsering this question, I created a new var named `GVS_t` and `GVS_a` (representing GVKEY source for both taregt and Acquiror); its value is:\n",
    "    \n",
    "| wrds found? | evans found? | 2 bridge match? | value (str) |\n",
    "|-------------|--------------|-----------------|-------|\n",
    "| 1           | 1            | 1               | 1     |\n",
    "| 1           | 1            | 0               | 2     |\n",
    "| 0           | 1            | 0               | 3     |\n",
    "| 1           | 0            | 0               | 4     |\n",
    "| 0           | 0            | 1               | 0     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T20:05:21.093555Z",
     "start_time": "2021-09-10T20:05:21.090534Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_data_path = './data/tmp'\n",
    "\n",
    "s_year = 1997\n",
    "e_year = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T20:05:23.761894Z",
     "start_time": "2021-09-10T20:05:21.332404Z"
    }
   },
   "outputs": [],
   "source": [
    "# load ori merged data\n",
    "\n",
    "if os.path.isfile(pjoin(tmp_data_path , f'merged_ori_{s_year}_{e_year}.pickle')):\n",
    "    merged_raw = pd.read_pickle(pjoin(tmp_data_path , f'merged_ori_{s_year}_{e_year}.pickle'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T20:05:24.021549Z",
     "start_time": "2021-09-10T20:05:23.769111Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_raw = merged_raw.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:39:17.328784Z",
     "start_time": "2021-09-10T02:39:17.325293Z"
    }
   },
   "source": [
    "Some notes:\n",
    "1.  always use `pd.notna` and `pd.isna` to judge! do not use `== np.nan` or `==np.NaN`\n",
    "2. `~pd.notna` is not equal to `pd.isna`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T20:05:24.036115Z",
     "start_time": "2021-09-10T20:05:24.028791Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def bridge_explore(row, **kwargs):\n",
    "    tmp_dict = {'t':'x', 'a':'y'}\n",
    "    if pd.notna(row['GVKEY_'+ tmp_dict[F]]) & pd.notna(row[F+'gvkey']): # both found\n",
    "        if row['GVKEY_'+ tmp_dict[F]] == row[F+'gvkey']:\n",
    "            return '1'\n",
    "        else:\n",
    "            return '2'\n",
    "    elif pd.isna(row['GVKEY_'+ tmp_dict[F]]) & pd.isna(row[F+'gvkey']): # both not found\n",
    "        return '0'\n",
    "    else:\n",
    "        if pd.notna(row['GVKEY_'+ tmp_dict[F]]): # wrds found\n",
    "            return '4'\n",
    "        else:\n",
    "            return '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T20:06:08.930416Z",
     "start_time": "2021-09-10T20:05:24.042772Z"
    }
   },
   "outputs": [],
   "source": [
    "for F in ['t', 'a']:\n",
    "    merged_raw['GVS_'+F] = merged_raw.apply(bridge_explore, F=F, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep in mind we want to make sure there are 2 matches for every obs; one from wrds, one from evans.\n",
    "- for evans_bridge, delete NAs (previously presented as `'-1'`)\n",
    "- for wrds_bridge, make sure the match is lying under the \"effective time period\".\n",
    "\n",
    "\n",
    "\n",
    "`anomaly_all`: both bridge found target gvkey, but gvkeys are not the same. (the `2` condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:47.312537Z",
     "start_time": "2021-09-11T15:54:47.125440Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract anomaly for target\n",
    "merged_raw_2_t = merged_raw[(merged_raw['tgvkey'] != '-1') & ((merged_raw['LINKDT_x'] <= merged_raw['DA']) & (merged_raw['LINKENDDT_x'] >= merged_raw['DA']))] \n",
    "anomaly_all_t = merged_raw_2_t[(merged_raw_2_t['GVS_t'] == '2') ][['TCU', 'TNL', 'TPUBC', 'TUP', 'TUPNAMES', 'TUPPUB', 'GVKEY_x','tgvkey', 'DEAL_NO']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:47.709345Z",
     "start_time": "2021-09-11T15:54:47.345812Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract anomaly for acquiror\n",
    "\n",
    "merged_raw_2_a = merged_raw[(merged_raw['agvkey'] != '-1') & ((merged_raw['LINKDT_y'] <= merged_raw['DA']) & (merged_raw['LINKENDDT_y'] >= merged_raw['DA']))] \n",
    "anomaly_all_a = merged_raw_2_a[(merged_raw_2_a['GVS_a'] == '2') ][['ACU', 'ANL', 'APUBC', 'AUP', 'AUPNAMES', 'AUPPUB', 'GVKEY_y','agvkey', 'DEAL_NO']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we focus on 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:48.143786Z",
     "start_time": "2021-09-11T15:54:48.139843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the table that wrds and evans bridge linkings give different target gvkey matchs :  (74, 9) \n",
      "\n",
      "the table that wrds and evans bridge linkings give different acquiror gvkey matchs :  (224, 9)\n"
     ]
    }
   ],
   "source": [
    "print('the table that wrds and evans bridge linkings give different target gvkey matchs : ', anomaly_all_t.shape, '\\n')\n",
    "print('the table that wrds and evans bridge linkings give different acquiror gvkey matchs : ', anomaly_all_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:48.361031Z",
     "start_time": "2021-09-11T15:54:48.349095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GVS_a condition info:\n",
      " 4    19288\n",
      "0     3885\n",
      "3     3686\n",
      "1     3669\n",
      "2      237\n",
      "Name: GVS_a, dtype: int64 \n",
      "\n",
      "GVS_t condition info:\n",
      " 4    23173\n",
      "1     7518\n",
      "2       74\n",
      "Name: GVS_t, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('GVS_a condition info:\\n', merged_raw_2['GVS_a'].value_counts(),'\\n')\n",
    "print('GVS_t condition info:\\n', merged_raw_2['GVS_t'].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`anomaly_notsame`: the cusip of the `target` and the `parent of the target` is different\n",
    "\n",
    "`anomaly_same`: ~`anomaly_notsame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:49.426237Z",
     "start_time": "2021-09-11T15:54:49.423873Z"
    }
   },
   "outputs": [],
   "source": [
    "# anomaly_notsame_t = anomaly_all_t[anomaly_all_t.TCU != anomaly_all_t.TUP]\n",
    "# anomaly_notsame_a = anomaly_all_a[anomaly_all_a.ACU != anomaly_all_a.AUP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:49.616313Z",
     "start_time": "2021-09-11T15:54:49.614108Z"
    }
   },
   "outputs": [],
   "source": [
    "# anomaly_same_t = anomaly_all_t[anomaly_all_t.TCU == anomaly_all_t.TUP]\n",
    "# anomaly_same_a = anomaly_all_a[anomaly_all_a.ACU == anomaly_all_a.AUP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:49.845736Z",
     "start_time": "2021-09-11T15:54:49.843590Z"
    }
   },
   "outputs": [],
   "source": [
    "# anomaly_notsame.to_csv('anomaly_notsame.csv')\n",
    "# anomaly_same.to_csv('anomaly_same.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "\n",
    "\n",
    "\n",
    "### Case 1\n",
    "\n",
    "target and ultimate parent are the same according to CUSIP\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/9eJ6mpO4DsBsTU3M_H8inTcUPz3jW3ozX385_HdxWJE.original.fullsize.png)\n",
    "\n",
    "Both the WRDS match and EVANS match are wrong.\n",
    "\n",
    "The Stober Organization Inc is a [private firm ](https://www.crunchbase.com/organization/the-strober-organization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/axFN-Q-QCzFwc8xtHUw7fsphNM-MKhbhcSItL6O7sMw.original.fullsize.png)\n",
    "\n",
    "`Interactive Group` is also a private firm according to cruchbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by maunally check the firm names\n",
    "\n",
    "\n",
    "I retreive the firm name `conm` via `GVKEY_x` and `tgvkey` (from wrds linking and evans linking) from compustat. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:29:21.746493Z",
     "start_time": "2021-09-11T15:29:13.384191Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your WRDS username [dalab5]:dayuyang1999\n",
      "Enter your password:Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "WRDS recommends setting up a .pgpass file.\n",
      "Create .pgpass file now [y/n]?: y\n",
      "Created .pgpass file successfully.\n",
      "Loading library list...\n",
      "Done\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "db = wrds.Connection()\n",
    "db = wrds.Connection(wrds_username='dayuyang1999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:51.862749Z",
     "start_time": "2021-09-11T15:54:51.857311Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_true_firmname(df, key):\n",
    "    '''\n",
    "    args:\n",
    "        df: ori data df \n",
    "        key: string, var name contains gvkey\n",
    "    return: df with a new var read_firm_name\n",
    "    \n",
    "    '''\n",
    "    gvkeys = df[key].astype(str).to_list()\n",
    "    columns = ['gvkey','conm']\n",
    "    \n",
    "    query = build_query(dataset = 'comp.funda',\n",
    "                   columns = columns,\n",
    "                   gvkey = gvkeys,\n",
    "                   start = '1997-01-01',\n",
    "                   end='2021-01-01')\n",
    "    gvkey_tmp = db.raw_sql(query)\n",
    "    gvkey_tmp.drop_duplicates(subset=['gvkey','conm'], keep='last')\n",
    "    df2 = df.merge(gvkey_tmp, left_on = key, right_on = \"gvkey\", how = \"left\")\n",
    "    df2 = df2.drop('gvkey', 1)\n",
    "    df2 = df2.rename(columns={'conm':key+'compustat_firmname'})\n",
    "    return df2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:52.106103Z",
     "start_time": "2021-09-11T15:54:52.057679Z"
    }
   },
   "outputs": [],
   "source": [
    "# match name for target anomaly\n",
    "df_merged_t_1 = add_true_firmname(anomaly_all_t, 'GVKEY_x')\n",
    "df_merged_t_2 = add_true_firmname(df_merged_t_1, 'tgvkey')\n",
    "df_merged_t_2 = df_merged_t_2.drop_duplicates(subset=['TCU','TNL','TUP','TUPNAMES', 'GVKEY_xcompustat_firmname', 'tgvkeycompustat_firmname'], keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:52.303569Z",
     "start_time": "2021-09-11T15:54:52.256902Z"
    }
   },
   "outputs": [],
   "source": [
    "# match name for acquirer anomaly\n",
    "df_merged_a_1 = add_true_firmname(anomaly_all_a, 'GVKEY_y')\n",
    "df_merged_a_2 = add_true_firmname(df_merged_a_1, 'agvkey')\n",
    "df_merged_a_2 = df_merged_a_2.drop_duplicates(subset=['ACU','ANL','AUP','AUPNAMES','GVKEY_ycompustat_firmname', 'agvkeycompustat_firmname'], keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:52.486077Z",
     "start_time": "2021-09-11T15:54:52.483228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 11)\n",
      "(58, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df_merged_t_2.shape)\n",
    "print(df_merged_a_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:54:54.805363Z",
     "start_time": "2021-09-11T15:54:54.800700Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged_t_2.to_csv('check_q1_target.csv')\n",
    "df_merged_a_2.to_csv('check_q1_acquiror.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For target\n",
    "\n",
    "manually check the csv file.\n",
    "- why manually? \n",
    "    - since the name of firm is hard to match(should be pre-processed if check by code)\n",
    "\n",
    "If there is: the firm name from compustat `GVKEY_xcompustat_firmname`\tor `tgvkeycompustat_firmname`:\n",
    "1. do not match to `TNL`, but match `TUPNAMES`\n",
    "    - which means the linking use ultimate parent as substitution of original target\n",
    "1. do not match to either `TNL` or `TUPNAMES`\n",
    "    - which means the match is wrong\n",
    "\n",
    "\n",
    "\n",
    "**findings**:\n",
    "\n",
    "##### 1st\n",
    "\n",
    "for condition 1 above, I do find 2 instances:\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/7wxItivAjSwPm3OWENrkHWT6C3409g2xpyaVhGFyarQ.original.fullsize.png)\n",
    "\n",
    "- see deal `2722161020`, wrds linking gives us the gvkey of `GREAT WOLF RESORTS INC`, which is the correct target. However, evans linking gives us the gvkey of its ultimate parent which is `APOLLO GLOBAL MGMT INC`, this is wrong.\n",
    "\n",
    "- see deal `2797838020`, again, wrds linking is correct.\n",
    "\n",
    "\n",
    "\n",
    "##### 2nd\n",
    "\n",
    "lots of obs lies on condition 2, that means both the wrds and evans linkings give us wrong gvkey matchs. So we cannot find info corresponding to those gvkeys in compustat database.\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/-OJdW04mtZReoPkQ9rxcfsRNNfiPEtkVx1CiAu_1nVQ.original.fullsize.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For acquiror\n",
    "\n",
    "situation are similar to target, for example, see deal `1623900020`:\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/INiIuqw0sr7xaZWGal8CP5Sy1ThszPVetLBWWGHTxsQ.original.fullsize.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Q1\n",
    "\n",
    "1. we should trust WRDS linking more than EVANS linking\n",
    "    - so if both give gvkey match, use the one from `wrds`\n",
    "    - if only one gives, use that one (better than nothing)\n",
    "\n",
    "2. sometimes, the WRDS or EVANS linking gives us wrong GVKEY matches\n",
    "    - after retreving financial variables from Compustat (using the gvkey wrds or evans linking gives us), simply drop those NAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: at least one is Public, then analysisable\n",
    "\n",
    "\n",
    "\n",
    "- Although `GVKEY` is mostly for public firm, but some private firm do have `GVKEY` as well.\n",
    "- However, TNIC would only cover Public firms.\n",
    "\n",
    "Which means, we could only analysis the following conditions:\n",
    "\n",
    "| APUBC == Public? | AUPPUB == Public? | TPUBC == Public? | TUPPUB == Public? | mark as                                           |\n",
    "|------------------|-------------------|------------------|-------------------|---------------------------------------------------|\n",
    "| 1                | 1                 | 1                | 1                 | 1                                                 |\n",
    "| 1                | 1                 | 1                | 0                 | 2                                                 |\n",
    "| 1                | 1                 | 0                | 1                 | 3                                                 |\n",
    "| 1                | 0                 | 1                | 1                 | 4                                                 |\n",
    "| 1                | 0                 | 1                | 0                 | 5                                                 |\n",
    "| 1                | 0                 | 0                | 1                 | 6                                                 |\n",
    "| 0                | 1                 | 1                | 1                 | 7                                                 |\n",
    "| 0                | 1                 | 1                | 0                 | 8                                                 |\n",
    "| 0                | 1                 | 0                | 1                 | 9                                                 |\n",
    "|                  |                   |                  |                   | all other combination is unanalysiable, mark as 0 |\n",
    "\n",
    "\n",
    "We need to analysis the ratio first before deciding how to manipulate the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:59:28.795817Z",
     "start_time": "2021-09-13T23:59:26.887097Z"
    }
   },
   "outputs": [],
   "source": [
    "sdc_df = pd.read_pickle(pjoin(tmp_data_path , f'sdc_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:59:28.815607Z",
     "start_time": "2021-09-13T23:59:28.799270Z"
    }
   },
   "outputs": [],
   "source": [
    "def mark_if_public(row, **kwargs):\n",
    "    if (row.APUBC == 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '1'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB != 'Public'):\n",
    "        return '2'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC != 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '3'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB != 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '4'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB != 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB != 'Public'):\n",
    "        return '5'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB != 'Public') & (row.TPUBC != 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '6'\n",
    "    elif (row.APUBC != 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '7'\n",
    "    elif (row.APUBC != 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB != 'Public'):\n",
    "        return '8'\n",
    "    elif (row.APUBC != 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC != 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '9'\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T00:01:54.527559Z",
     "start_time": "2021-09-13T23:59:36.598739Z"
    }
   },
   "outputs": [],
   "source": [
    "sdc_df['PUB_COND'] = sdc_df.apply(mark_if_public, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T00:01:54.629902Z",
     "start_time": "2021-09-14T00:01:54.530752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    222921\n",
      "1     27358\n",
      "3     14550\n",
      "9      5818\n",
      "7      1141\n",
      "5       482\n",
      "6       404\n",
      "4       249\n",
      "2       189\n",
      "8        38\n",
      "Name: PUB_COND, dtype: int64 \n",
      " Ratios : \n",
      " 0    81.611203\n",
      "1    10.015742\n",
      "3     5.326744\n",
      "9     2.129965\n",
      "7     0.417719\n",
      "5     0.176460\n",
      "6     0.147904\n",
      "4     0.091159\n",
      "2     0.069193\n",
      "8     0.013912\n",
      "Name: PUB_COND, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(sdc_df.PUB_COND.value_counts(), '\\n', f'Ratios : \\n {sdc_df.PUB_COND.value_counts()/sdc_df.shape[0]*100}')\n",
    "#print(merged.PUB_COND.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, mostly are 3, 1, 9, 0, 7.\n",
    "\n",
    "## `3` and `9`\n",
    "`3` : A public acquiror acquires a non-public firm but its parent firm is a public firm(..see table for better understanding).\n",
    "\n",
    "We could see mostly are `Sub.`:\n",
    "\n",
    "- `Subsidiary`: According to [investopedia](https://www.investopedia.com/terms/t/takeover.asp), After the 50% threshold has been breached, the target company should be considered a subsidiary.\n",
    "\n",
    "\n",
    "So **I think(should find paper for endorcement)** `3` is a type of takeover that `ACU`(a Public firm) acquiror \"a small portion of\" the `TUP` (**Pay attention to the abrrev!**).\n",
    "\n",
    "- Ok, I found the endorcement, see [Jaffe, Jeffrey, Jan Jindra, David Pedersen, and Torben Voetmann. \"Returns to acquirers of public and subsidiary targets.\" Journal of Corporate Finance 31 (2015): 246-270.](https://reader.elsevier.com/reader/sd/pii/S0929119915000310?token=7DD9D17E9A100FD0A9EFE2E85EA5F6AE17E572DE2A7D4488FF8AD400E526C366CA1D05ADBC7A123590D379757B64A6B7&originRegion=us-east-1&originCreation=20210911194358)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T19:22:41.242923Z",
     "start_time": "2021-09-11T19:22:41.165847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sub.     6957\n",
       "J.V.      157\n",
       "Priv.      92\n",
       "Govt.       2\n",
       "Name: TPUBC, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sdc_df[(sdc_df.APUBC == 'Public') &  (sdc_df.AUPPUB == 'Public') & (sdc_df.TPUBC != 'Public') & (sdc_df.TUPPUB == 'Public')].TPUBC.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T19:29:55.369941Z",
     "start_time": "2021-09-11T19:29:55.354398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sub.     2151\n",
       "J.V.       42\n",
       "Priv.      14\n",
       "Govt.       2\n",
       "Name: TPUBC, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[(sdc_df.APUBC != 'Public') &  (sdc_df.AUPPUB == 'Public') & (sdc_df.TPUBC != 'Public') & (sdc_df.TUPPUB == 'Public')].TPUBC.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1` and `7`\n",
    "\n",
    "`1`: Just Normal Condition, no need to worry about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Only Consider Complete?\n",
    "\n",
    "Observations:\n",
    "1. So far, all MA quantitiative analysis research I read only consider complete transactions...\n",
    "2. Withdrawn bid may be valuable to cindiser...\n",
    "\n",
    "Definition:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T22:42:15.789618Z",
     "start_time": "2021-09-11T22:42:15.660048Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_public = pd.read_pickle(pjoin(tmp_data_path , f'merged_public_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T22:42:33.994686Z",
     "start_time": "2021-09-11T22:42:33.987807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C     13893\n",
       "P      1161\n",
       "W      1039\n",
       "UN      415\n",
       "DR       88\n",
       "I        45\n",
       "R         5\n",
       "IW        2\n",
       "PC        1\n",
       "Name: STATC, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_public.STATC.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8cddd8541f7e6b584fe97d7036d57f0700709042cc565c9ada9a5070d4704dc"
  },
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
