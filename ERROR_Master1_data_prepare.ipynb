{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:40:25.047029Z",
     "start_time": "2021-09-11T16:40:24.558077Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime \n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "#import argparse\n",
    "#import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:23:00.220803Z",
     "start_time": "2021-09-10T18:22:59.526138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/dalab5/Projects/MA_refined', '/home/dalab5/miniconda3/envs/GNN/lib/python38.zip', '/home/dalab5/miniconda3/envs/GNN/lib/python3.8', '/home/dalab5/miniconda3/envs/GNN/lib/python3.8/lib-dynload', '', '/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages', '/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/IPython/extensions', '/home/dalab5/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# # if package is not properly loaded, check env path \n",
    "# import sys\n",
    "\n",
    "# print(sys.path)\n",
    "\n",
    "# from openpyxl import Workbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T01:38:36.053851Z",
     "start_time": "2021-09-10T01:38:36.050372Z"
    }
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='MA basic setting')\n",
    "# parser.add_argument('--config', default='./config.yaml')\n",
    "\n",
    "# #global args\n",
    "# args = parser.parse_args()\n",
    "# with open(args.config) as f:\n",
    "#     config = yaml.load(f)\n",
    "    \n",
    "# for key in config:\n",
    "#     for k, v in config[key].items():\n",
    "#         setattr(args, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:40:38.870512Z",
     "start_time": "2021-09-11T16:40:38.867930Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_data_path =  './data/tmp'\n",
    "s_year = 1997\n",
    "e_year = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data\n",
    "\n",
    "3 groups of data\n",
    "- bridge 1: wrds bridge\n",
    "- bridge 2: evans bridge\n",
    "- SDC MA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:24.654303Z",
     "start_time": "2021-09-10T02:56:24.511650Z"
    }
   },
   "outputs": [],
   "source": [
    "# bridge 1\n",
    "wrds_bridge = pd.read_csv('./data/wrds_bridge.csv', header=0, engine=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:24.731177Z",
     "start_time": "2021-09-10T02:56:24.726774Z"
    }
   },
   "outputs": [],
   "source": [
    "# rename \n",
    "\n",
    "wrds_bridge.rename(columns={x: x.replace(' ', '_').upper().strip() for x in wrds_bridge.columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:24.918681Z",
     "start_time": "2021-09-10T02:56:24.908095Z"
    }
   },
   "outputs": [],
   "source": [
    "# for now, we just need 4 columns\n",
    "\n",
    "wrds_bridge = wrds_bridge[['CUSIP', 'GVKEY', 'LINKDT', 'LINKENDDT', 'CONM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:25.302495Z",
     "start_time": "2021-09-10T02:56:25.287827Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSIP</th>\n",
       "      <th>GVKEY</th>\n",
       "      <th>LINKDT</th>\n",
       "      <th>LINKENDDT</th>\n",
       "      <th>CONM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18792</th>\n",
       "      <td>888303104</td>\n",
       "      <td>28576</td>\n",
       "      <td>08/31/1993</td>\n",
       "      <td>12/22/1997</td>\n",
       "      <td>TITAN HOLDINGS INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>165195108</td>\n",
       "      <td>2983</td>\n",
       "      <td>07/01/1973</td>\n",
       "      <td>08/30/1978</td>\n",
       "      <td>CHESAPEAKE INDUSTRIES INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25605</th>\n",
       "      <td>65332E101</td>\n",
       "      <td>120593</td>\n",
       "      <td>05/18/1999</td>\n",
       "      <td>11/28/2003</td>\n",
       "      <td>NEXTERA ENTERPRISES INC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CUSIP   GVKEY      LINKDT   LINKENDDT                       CONM\n",
       "18792  888303104   28576  08/31/1993  12/22/1997         TITAN HOLDINGS INC\n",
       "1964   165195108    2983  07/01/1973  08/30/1978  CHESAPEAKE INDUSTRIES INC\n",
       "25605  65332E101  120593  05/18/1999  11/28/2003    NEXTERA ENTERPRISES INC"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see data structure\n",
    "wrds_bridge.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:27.433313Z",
     "start_time": "2021-09-10T02:56:27.375662Z"
    }
   },
   "outputs": [],
   "source": [
    "# bridge 2\n",
    "evans_bridge = pd.read_csv('./data/evans_bridge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:27.572606Z",
     "start_time": "2021-09-10T02:56:27.557746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DealNumber</th>\n",
       "      <th>agvkey</th>\n",
       "      <th>tgvkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23238</th>\n",
       "      <td>789043040</td>\n",
       "      <td>1706.0</td>\n",
       "      <td>9173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95651</th>\n",
       "      <td>2320221020</td>\n",
       "      <td>177224.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>624668040</td>\n",
       "      <td>13197.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DealNumber    agvkey  tgvkey\n",
       "23238   789043040    1706.0  9173.0\n",
       "95651  2320221020  177224.0     NaN\n",
       "9544    624668040   13197.0     NaN"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evans_bridge.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:50:06.784255Z",
     "start_time": "2021-09-11T16:50:06.781302Z"
    }
   },
   "outputs": [],
   "source": [
    "# AF - BJ\n",
    "\n",
    "name_lst = [\n",
    "    'ACU', 'ASIC2', 'ABL', 'ANL', 'APUBC', 'AUP', 'AUPSIC', 'AUPBL', 'AUPNAMES', 'AUPPUB',\n",
    "    'BLOCK','CREEP','DA','DE','STATC','SYNOP','VAL','PCTACQ','PSOUGHTOWN','PSOUGHT','PHDA','PCTOWN','PSOUGHTT','PRIVATIZATION','DEAL_NO',\n",
    "    'TCU', 'TSIC2', 'TBL', 'TNL', 'TPUBC', 'TUP', 'TUPSIC', 'TUPBL', 'TUPNAMES', 'TUPPUB'    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before concat all data, manually convert all date on excel file to \"YY-mm-dd\" format.\n",
    "\n",
    "Guide: select all date var --> right-click --> Cell format --> Date --> 2012-03-14 --> OK\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/58hcVJ3qFlC446Ns4SaJKtm-UroEqUKyqu4oCnTWhKY.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T01:38:40.168977Z",
     "start_time": "2021-09-10T01:38:40.158615Z"
    }
   },
   "outputs": [],
   "source": [
    "def concat_data(st, end):\n",
    "    df_l = []\n",
    "    for year in range(st, end+1, 1):\n",
    "        df = pd.read_excel(f\"./data/SDC/{year}.xlsx\", header=1, engine='openpyxl')\n",
    "        #df = df.drop(df.columns[4], axis=1) # this column is duplicate with column 3 \n",
    "        #print(len(df.columns))\n",
    "        df.columns = name_lst\n",
    "        \n",
    "        # check date var loading ok\n",
    "        check = df[df['DA'] == datetime.time(0, 0)]\n",
    "        if check.shape[0] == 0 :\n",
    "            print('date variables loading ok \\n')\n",
    "        else:\n",
    "            print('date variables loading fail, please manually check. number of failed records: ', check.shape[0])\n",
    "        \n",
    "        df_l.append(df)\n",
    "        print(f'{year} data shape:', df.shape)\n",
    "        del df\n",
    "    df = pd.concat(df_l)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T01:40:19.341141Z",
     "start_time": "2021-09-10T01:38:40.371079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date variables loading ok \n",
      "\n",
      "1997 data shape: (13255, 35)\n",
      "date variables loading ok \n",
      "\n",
      "1998 data shape: (15081, 35)\n",
      "date variables loading ok \n",
      "\n",
      "1999 data shape: (13203, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2000 data shape: (12610, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2001 data shape: (8771, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2002 data shape: (7943, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2003 data shape: (8573, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2004 data shape: (9704, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2005 data shape: (10524, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2006 data shape: (11802, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2007 data shape: (12866, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2008 data shape: (11174, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2009 data shape: (8965, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2010 data shape: (9365, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2011 data shape: (9859, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2012 data shape: (9772, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2013 data shape: (10060, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2014 data shape: (11349, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2015 data shape: (11964, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2016 data shape: (12493, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2017 data shape: (14453, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2018 data shape: (13776, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2019 data shape: (12216, 35)\n",
      "date variables loading ok \n",
      "\n",
      "2020 data shape: (13372, 35)\n"
     ]
    }
   ],
   "source": [
    "sdc_df = concat_data(s_year, e_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T01:40:19.465388Z",
     "start_time": "2021-09-10T01:40:19.342462Z"
    }
   },
   "outputs": [],
   "source": [
    "sdc_df = sdc_df.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T01:40:20.540007Z",
     "start_time": "2021-09-10T01:40:19.467380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving sdc table ranging from 1997 to 2020 ...\n"
     ]
    }
   ],
   "source": [
    "print(f'saving sdc table ranging from {s_year} to {e_year} ...')\n",
    "sdc_df.to_pickle(pjoin(tmp_data_path , f'sdc_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:36.496623Z",
     "start_time": "2021-09-10T02:56:35.112990Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can run code start from here if merging sdf_df is unnecessary\n",
    "\n",
    "if os.path.isfile(pjoin(tmp_data_path , f'sdc_{s_year}_{e_year}.pickle')):\n",
    "    sdc_df = pd.read_pickle(pjoin(tmp_data_path , f'sdc_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparing\n",
    "\n",
    "1. delete obs either `ACU` or `TCU` in full_df is NAs (otherwise we cannot identify the participants of deals)\n",
    "2. match the format of: time, CUSIP, GVKEY to be the same across three dataset (otherwise may trigger error when merging tables)\n",
    "    - Time: Timestap\n",
    "    - CUSIP / GVKEY: string\n",
    "    - dealnum: string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  sdc data\n",
    "\n",
    "1. drop where either `ACU` or `TCU` is Nas\n",
    "1. fill DEAL_NO NAs to -1\n",
    "1. change all identifier to `str`; including: `ACU`, `AUP`, `TCU`, `TUP`, `DEAL_NO`, `GVKEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:37.400039Z",
     "start_time": "2021-09-10T02:56:36.751263Z"
    }
   },
   "outputs": [],
   "source": [
    "sdc_df = sdc_df.dropna(subset=['ACU','TCU']) # actually nothing drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:37.734761Z",
     "start_time": "2021-09-10T02:56:37.402474Z"
    }
   },
   "outputs": [],
   "source": [
    "# deal num\n",
    "sdc_df['DEAL_NO'] = sdc_df['DEAL_NO'].fillna(-1)\n",
    "sdc_df['DEAL_NO'] = sdc_df['DEAL_NO'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:37.937382Z",
     "start_time": "2021-09-10T02:56:37.736341Z"
    }
   },
   "outputs": [],
   "source": [
    "sdc_df['TCU']  = sdc_df['TCU'].astype('str')\n",
    "sdc_df['ACU']  = sdc_df['ACU'].astype('str')\n",
    "sdc_df['TUP']  = sdc_df['TUP'].astype('str')\n",
    "sdc_df['AUP']  = sdc_df['AUP'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bridges data\n",
    "\n",
    "1. match variable type for merging\n",
    "    1. for CUSIP/GVKEY/DEALNUM, all convert to `string`; \n",
    "        - do not keep 0s at front \n",
    "            - e.g. `002030` will be curtail to `2030`\n",
    "    2. for time, all convert to pandas `Timestamp` instance\n",
    "2. drop na or fill na\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evans_bridge\n",
    "\n",
    "for evans_brdge, load as float:\n",
    "1. fill na as -1\n",
    "2. convert all var to integer\n",
    "3. convert all var to string\n",
    "\n",
    "\n",
    "**so the GVKEY has no 0 at front**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:39.660934Z",
     "start_time": "2021-09-10T02:56:39.163955Z"
    }
   },
   "outputs": [],
   "source": [
    "evans_bridge.DealNumber = evans_bridge.DealNumber.fillna(-1)\n",
    "evans_bridge.tgvkey = evans_bridge.tgvkey.fillna(-1)\n",
    "evans_bridge.agvkey = evans_bridge.agvkey.fillna(-1)\n",
    "\n",
    "\n",
    "evans_bridge = evans_bridge.astype('int')\n",
    "evans_bridge = evans_bridge.astype('str')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wrds_bridge\n",
    "\n",
    "- `GVKEY` and `CUSIP`, load as int; so just convert to str\n",
    "- no need to worry about NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:40.785022Z",
     "start_time": "2021-09-10T02:56:40.756272Z"
    }
   },
   "outputs": [],
   "source": [
    "wrds_bridge[['GVKEY', 'CUSIP']] = wrds_bridge[['GVKEY', 'CUSIP']].applymap(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:41.377096Z",
     "start_time": "2021-09-10T02:56:40.963473Z"
    }
   },
   "outputs": [],
   "source": [
    "# covvert 9 digit to 6 digit; \n",
    "if len(wrds_bridge.CUSIP[0]) == 9:\n",
    "    wrds_bridge['CUSIP'] = wrds_bridge.apply(lambda row: row['CUSIP'][:6], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since there are lots of \"E\" in LINKENDDT(still effective presently), LINKENDDT was read as str;\n",
    "\n",
    "we replace \"E\" as '12/31/2099' (string type), and then convert to pandas timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:41.385148Z",
     "start_time": "2021-09-10T02:56:41.378776Z"
    }
   },
   "outputs": [],
   "source": [
    "wrds_bridge.LINKENDDT[wrds_bridge.LINKENDDT == 'E'] = '12/31/2099'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:41.697571Z",
     "start_time": "2021-09-10T02:56:41.586464Z"
    }
   },
   "outputs": [],
   "source": [
    "wrds_bridge['LINKENDDT'] = pd.to_datetime(wrds_bridge['LINKENDDT'],format = '%m/%d/%Y' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:42.162940Z",
     "start_time": "2021-09-10T02:56:42.138169Z"
    }
   },
   "outputs": [],
   "source": [
    "wrds_bridge['LINKDT'] = pd.to_datetime(wrds_bridge['LINKDT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store bridge to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge\n",
    "\n",
    "general merging strategy:\n",
    "1. we take wrds linking as the main linking bridge; evans as secondary.\n",
    "    1. first use wrds bridge to match GVKEY info to the original SDC data\n",
    "    2. then use evans_bridge to match GVKEY for those obs which unable to find GVKEY info from wrds_bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:47.014592Z",
     "start_time": "2021-09-10T02:56:46.999683Z"
    }
   },
   "outputs": [],
   "source": [
    "#before merge, check all merge variables are in the same type\n",
    "\n",
    "# gvkey, cusip match\n",
    "\n",
    "assert type(wrds_bridge.GVKEY[0]) == type(wrds_bridge.CUSIP[0]) == type(evans_bridge.agvkey[0]) \n",
    "\n",
    "assert type(evans_bridge.agvkey[0]) == type(evans_bridge.tgvkey[0]) == type(sdc_df.ACU[0]) == type(sdc_df.TCU[0])\n",
    "        \n",
    "# deal number match\n",
    "\n",
    "assert type(sdc_df.DEAL_NO[0]) == type(evans_bridge.DealNumber[0])\n",
    "\n",
    "# time match\n",
    "\n",
    "assert type(sdc_df.DA[0]) == type(sdc_df.DE[0]) == type(wrds_bridge.LINKDT[0]) == type(wrds_bridge.LINKENDDT[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obtain GVKEY from linkings\n",
    "\n",
    "Rules:\n",
    "1. use WRDS linking table as primary table, EVANS as secondary\n",
    "2. drop those no gvkey was returned\n",
    "\n",
    "\n",
    "reason for rules refering to [Appendix 1](./Appendix1_data_explore.ipynb)\n",
    "\n",
    "+ save `merged_2` to pickle before checking out Appendix 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:49.362659Z",
     "start_time": "2021-09-10T02:56:47.675759Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1st merge with wrds bridge\n",
    "merged_1_1 = wrds_bridge.merge(sdc_df, left_on='CUSIP', right_on = 'ACU', how = 'right')\n",
    "merged_1_2 = wrds_bridge.merge(merged_1_1, left_on='CUSIP', right_on = 'TCU', how = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since math Acquiror before target, here `CUSIP_x` means target's CUSIP; \n",
    "\n",
    "`y` == acquirer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:49.925156Z",
     "start_time": "2021-09-10T02:56:49.364566Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2nd merge with evans bridge\n",
    "merged_2 = evans_bridge.merge(merged_1_2, left_on='DealNumber', right_on='DEAL_NO', how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:51.144403Z",
     "start_time": "2021-09-10T02:56:51.141158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300643, 48)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:51.969155Z",
     "start_time": "2021-09-10T02:56:51.756542Z"
    }
   },
   "outputs": [],
   "source": [
    "del merged_1_1, merged_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T02:56:54.628661Z",
     "start_time": "2021-09-10T02:56:52.090276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving original GVKEY merged table ranging from 1997 to 2020 ...\n"
     ]
    }
   ],
   "source": [
    "# save merged data for further exploration\n",
    "print(f'saving original GVKEY merged table ranging from {s_year} to {e_year} ...')\n",
    "merged_2.to_pickle(pjoin(tmp_data_path , f'merged_ori_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:40:50.378180Z",
     "start_time": "2021-09-11T16:40:48.324260Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can run code start from here \n",
    "\n",
    "if os.path.isfile(pjoin(tmp_data_path , f'merged_ori_{s_year}_{e_year}.pickle')):\n",
    "    merged_2 = pd.read_pickle(pjoin(tmp_data_path , f'merged_ori_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering:\n",
    "1. Target and Acquirer must at least successfully match with one table\n",
    "1. for those records (acquirer or target) that evans bridge did not much successfully, but wrds bridge matched successfully, confirm the `date accounced` of the deal falls in the effective time period of the linking. \n",
    "\n",
    "    why use `DA` instead of `DE`? since `DA` has less Nas\n",
    "    \n",
    "    \n",
    "* I chose to use wrds to be my primary linking. If two linking both matched, I will choose the result from wrds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:46:51.911696Z",
     "start_time": "2021-09-11T16:46:51.038340Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_raw = merged_2.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define some conditions:\n",
    "\n",
    "define a new variable: `LINKING_T` and `LINKING_A`\n",
    "\n",
    "| WRDS has match | WRDS match in valid time | EVANS has match | LINKING |\n",
    "|----------------|--------------------------|-----------------|---------|\n",
    "| 1              | 1                        | 1               | 1       |\n",
    "| 1              | 0                        | 1               | 1       |\n",
    "| 0              | 0                        | 1               | 1       |\n",
    "| 1              | 1                        | 0               | 1       |\n",
    "| 0              | 0                        | 0               | 0       |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:46:52.776961Z",
     "start_time": "2021-09-11T16:46:52.645363Z"
    }
   },
   "outputs": [],
   "source": [
    "# taregt gvkey\n",
    "\n",
    "cond_t = ( (merged_raw['GVKEY_x'].notnull() ) & (merged_raw['LINKDT_x'] <= merged_raw['DA']) & (merged_raw['LINKENDDT_x'] >= merged_raw['DA']) ) | ((merged_raw['tgvkey'].notnull()) & (merged_raw['tgvkey'] != '-1')) \n",
    "\n",
    "merged_raw['LINKING_T'] = np.where(cond_t, '1', '0') # create a new column first \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:48:09.399977Z",
     "start_time": "2021-09-11T16:48:09.300841Z"
    }
   },
   "outputs": [],
   "source": [
    "# acquiror gvkey\n",
    "\n",
    "cond_a =  ( (merged_raw['GVKEY_y'].notnull()) & (merged_raw['LINKDT_y'] <= merged_raw['DA']) & (merged_raw['LINKENDDT_y'] >= merged_raw['DA']) ) | ((merged_raw['agvkey'].notnull()) & (merged_raw['agvkey'] != '-1')) \n",
    "\n",
    "merged_raw['LINKING_A'] = np.where(cond_a, '1', '0') # create a new column first \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:53:03.499966Z",
     "start_time": "2021-09-11T16:53:03.248253Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:52:58.776330Z",
     "start_time": "2021-09-11T16:52:58.772305Z"
    }
   },
   "outputs": [],
   "source": [
    "# wrds as primary, evans as secondary\n",
    "\n",
    "def get_tgvkey(row):\n",
    "    # for target\n",
    "    if pd.notna(row['GVKEY_x']):\n",
    "        return row['GVKEY_x']        \n",
    "    elif pd.notna(row['tgvkey']):\n",
    "        return row['tgvkey']\n",
    "    else: \n",
    "        print('something is wrong')\n",
    "        \n",
    " \n",
    "\n",
    "def get_agvkey(row):\n",
    "    # for target\n",
    "    if pd.notna(row['GVKEY_y']):\n",
    "        return row['GVKEY_y']        \n",
    "    elif pd.notna(row['agvkey']):\n",
    "        return row['agvkey']\n",
    "    else:\n",
    "        print('something is wrong')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:53:23.949842Z",
     "start_time": "2021-09-11T16:53:19.355143Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-41749d380e89>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_bothok['TGVKEY'] = merged_bothok.apply(get_tgvkey, axis=1)\n",
      "<ipython-input-38-41749d380e89>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_bothok['AGVKEY'] = merged_bothok.apply(get_agvkey, axis=1)\n"
     ]
    }
   ],
   "source": [
    "merged_bothok = merged_raw[(merged_raw['LINKING_A'] == '1') & (merged_raw['LINKING_T'] == '1')]\n",
    "\n",
    "merged_bothok['TGVKEY'] = merged_bothok.apply(get_tgvkey, axis=1)\n",
    "merged_bothok['AGVKEY'] = merged_bothok.apply(get_agvkey, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:53:32.287108Z",
     "start_time": "2021-09-11T16:53:32.284204Z"
    }
   },
   "outputs": [],
   "source": [
    "new_name_lst = name_lst + ['AGVKEY', 'TGVKEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:53:34.311807Z",
     "start_time": "2021-09-11T16:53:34.091595Z"
    }
   },
   "outputs": [],
   "source": [
    "merged = merged_bothok[new_name_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T01:46:28.103448Z",
     "start_time": "2021-09-10T01:46:28.078976Z"
    }
   },
   "outputs": [],
   "source": [
    "# merged = merged[ ((merged['AGVKEY'].notnull()) & (merged['TGVKEY'].notnull()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove those self merge with self**\n",
    "\n",
    "\n",
    "I am sure we should do that!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:54:20.403007Z",
     "start_time": "2021-09-11T16:54:20.321260Z"
    }
   },
   "outputs": [],
   "source": [
    "merged = merged[merged['AGVKEY'] != merged['TGVKEY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T01:46:29.076395Z",
     "start_time": "2021-09-10T01:46:29.038573Z"
    }
   },
   "outputs": [],
   "source": [
    "del merged_bothok, merged_targetok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:54:50.740658Z",
     "start_time": "2021-09-11T16:54:50.361575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered merged data using data from 1997 to 2020 =  (17902, 37)\n"
     ]
    }
   ],
   "source": [
    "print(f'filtered merged data using data from {s_year} to {e_year} = ', merged.shape)\n",
    "\n",
    "merged.to_pickle(pjoin(tmp_data_path , f'merged_filtered_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T16:55:09.546125Z",
     "start_time": "2021-09-11T16:55:09.541221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ACU', 'ASIC2', 'ABL', 'ANL', 'APUBC', 'AUP', 'AUPSIC', 'AUPBL',\n",
       "       'AUPNAMES', 'AUPPUB', 'BLOCK', 'CREEP', 'DA', 'DE', 'STATC', 'SYNOP',\n",
       "       'VAL', 'PCTACQ', 'PSOUGHTOWN', 'PSOUGHT', 'PHDA', 'PCTOWN', 'PSOUGHTT',\n",
       "       'PRIVATIZATION', 'DEAL_NO', 'TCU', 'TSIC2', 'TBL', 'TNL', 'TPUBC',\n",
       "       'TUP', 'TUPSIC', 'TUPBL', 'TUPNAMES', 'TUPPUB', 'AGVKEY', 'TGVKEY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T19:35:06.816223Z",
     "start_time": "2021-09-11T19:35:06.808107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C     14951\n",
       "P      1246\n",
       "W      1119\n",
       "UN      439\n",
       "DR       91\n",
       "I        48\n",
       "R         5\n",
       "IW        2\n",
       "PC        1\n",
       "Name: STATC, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.STATC.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Takeover\n",
    "\n",
    "\n",
    "## Public?\n",
    "- Although `GVKEY` is mostly for public firm, but some private firm do have `GVKEY` as well.\n",
    "- However, TNIC would only cover Public firms.\n",
    "\n",
    "Which means, we could only analysis the following conditions:\n",
    "\n",
    "| APUBC == Public? | AUPPUB == Public? | TPUBC == Public? | TUPPUB == Public? | mark as                                           |\n",
    "|------------------|-------------------|------------------|-------------------|---------------------------------------------------|\n",
    "| 1                | 1                 | 1                | 1                 | 1                                                 |\n",
    "| 1                | 1                 | 1                | 0                 | 2                                                 |\n",
    "| 1                | 1                 | 0                | 1                 | 3                                                 |\n",
    "| 1                | 0                 | 1                | 1                 | 4                                                 |\n",
    "| 1                | 0                 | 1                | 0                 | 5                                                 |\n",
    "| 1                | 0                 | 0                | 1                 | 6                                                 |\n",
    "| 0                | 1                 | 1                | 1                 | 7                                                 |\n",
    "| 0                | 1                 | 1                | 0                 | 8                                                 |\n",
    "| 0                | 1                 | 0                | 1                 | 9                                                 |\n",
    "|                  |                   |                  |                   | all other combination is unanalysiable, mark as 0 |\n",
    "\n",
    "The analysis the ratio refer to [Appendix 1](./Appendix1_data_explore.ipynb)\n",
    "\n",
    "**Conclusion**\n",
    "- `3` and `9` are one type of MA\n",
    "- `1` and `7` are another type of MA\n",
    "- drop others\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T21:06:09.057169Z",
     "start_time": "2021-09-11T21:06:09.048917Z"
    }
   },
   "outputs": [],
   "source": [
    "def mark_if_public(row, **kwargs):\n",
    "    if (row.APUBC == 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '1'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB != 'Public'):\n",
    "        return '2'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC != 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '3'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB != 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '4'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB != 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB != 'Public'):\n",
    "        return '5'\n",
    "    elif (row.APUBC == 'Public') &  (row.AUPPUB != 'Public') & (row.TPUBC != 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '6'\n",
    "    elif (row.APUBC != 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '7'\n",
    "    elif (row.APUBC != 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC == 'Public') & (row.TUPPUB != 'Public'):\n",
    "        return '8'\n",
    "    elif (row.APUBC != 'Public') &  (row.AUPPUB == 'Public') & (row.TPUBC != 'Public') & (row.TUPPUB == 'Public'):\n",
    "        return '9'\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T21:06:19.636806Z",
     "start_time": "2021-09-11T21:06:14.883142Z"
    }
   },
   "outputs": [],
   "source": [
    "merged['PUB_COND'] = merged.apply(mark_if_public, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T21:08:11.775191Z",
     "start_time": "2021-09-11T21:08:11.719757Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_public = merged[merged['PUB_COND'].isin(['1','3','7','9'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T21:09:19.207232Z",
     "start_time": "2021-09-11T21:09:19.032103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle merged data with public info from 1997 to 2020 =  (16649, 38)\n"
     ]
    }
   ],
   "source": [
    "print(f'pickle merged data with public info from {s_year} to {e_year} = ', merged_public.shape)\n",
    "\n",
    "merged_public.to_pickle(pjoin(tmp_data_path , f'merged_public_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 STATC == Completed\n",
    "\n",
    "\n",
    "Consider `Complete` and `Withdrawn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T22:45:55.402413Z",
     "start_time": "2021-09-11T22:45:55.351305Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_modeling = merged_public[merged_public.STATC.isin(['C', 'W'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add Year and SIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T20:18:04.127013Z",
     "start_time": "2021-09-12T20:18:04.121457Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sic(df):\n",
    "    '''\n",
    "    df: the sdc table contains sic variable named as `ASIC2`\n",
    "    \n",
    "    '''\n",
    "    x = df.ASIC2.str.split('/')\n",
    "    x = x.transform(lambda x: x[0] if not isinstance(x, float) else np.nan)\n",
    "    df['SIC_A'] = x\n",
    "\n",
    "    x = df.ASIC2.str.split('/')\n",
    "    x = x.transform(lambda x: x[0] if not isinstance(x, float) else np.nan)\n",
    "    df['SIC_T'] = x\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T18:09:31.157737Z",
     "start_time": "2021-09-12T18:09:31.153159Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_modeling[\"YEAR\"] = merged_modeling.DA.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T20:18:17.339199Z",
     "start_time": "2021-09-12T20:18:17.167173Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_modeling = get_sic(merged_modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T20:18:58.963898Z",
     "start_time": "2021-09-12T20:18:58.661489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle merged data to model from 1997 to 2020 =  (14932, 41)\n"
     ]
    }
   ],
   "source": [
    "print(f'pickle merged data to model from {s_year} to {e_year} = ', merged_modeling.shape)\n",
    "\n",
    "merged_modeling.to_pickle(pjoin(tmp_data_path , f'merged_model_{s_year}_{e_year}.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8cddd8541f7e6b584fe97d7036d57f0700709042cc565c9ada9a5070d4704dc"
  },
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
