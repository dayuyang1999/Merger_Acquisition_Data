{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:02:16.969457Z",
     "start_time": "2021-11-03T16:02:00.398108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before removing same-day multi events: (9661, 50)\n",
      "shape after removing same-day multi events: (9448, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### creating step 1 data #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.76s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### creating step 2 data #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:05<00:05,  5.38s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.15s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "# cpu multitasking\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from data_preprocess import dataloader_preproceser, same_day_only_one,  minmax_normalize \n",
    "######################################### creating global variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## helpers\n",
    "\n",
    "def same_day_only_one(sdc_tnic_raw):\n",
    "    print(\"shape before removing same-day multi events:\", sdc_tnic_raw.shape)\n",
    "    sdc_tnic_raw = sdc_tnic_raw.copy()\n",
    "    sdc_tnic_one = sdc_tnic_raw.groupby(['AGVKEY', 'DA']).first().reset_index(drop=False)\n",
    "    print(\"shape after removing same-day multi events:\", sdc_tnic_one.shape)\n",
    "    sdc_tnic_one.sort_values(by = ['DA'], axis=0, inplace=True)\n",
    "    return sdc_tnic_one\n",
    "\n",
    "def N01_normalize(df):\n",
    "    '''\n",
    "    df could be df or array\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    normalized_df=(df-df.mean())/df.std()\n",
    "    return normalized_df\n",
    "\n",
    "def minmax_normalize(df):\n",
    "    df = df.copy()\n",
    "    normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "    return normalized_df\n",
    "\n",
    "#@dask.delayed\n",
    "def dataloader_preproceser(focal_gvkey):\n",
    "    '''\n",
    "    WARNING: this function requires some global variables that does not explicitly written in argument.\n",
    "            so the location of this function in the code cannot be changed.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def get_arr_c(focal_gvkey):\n",
    "    # part 1, get c  \n",
    "        def get_focal_df(focal_gvkey):\n",
    "            '''\n",
    "            output: will be a df contains 3 columns: DATE, AGVKEY, EVENT_TYPE, SCORE. TGVKEY\n",
    "                DATE: datetime.dt object\n",
    "                AGVKEY: str: 4 - 6 digits\n",
    "                EVENT_TYPE: 1:self 0:peer (integer)\n",
    "                SCORE: TNIC similarity last year for event type 0, otherwise 1\n",
    "            \n",
    "            Use DA!!!! not DE\n",
    "\n",
    "            '''\n",
    "            def helper1(row):\n",
    "                if row.AGVKEY == focal_gvkey:\n",
    "                    return 1 # integer 1\n",
    "                else:\n",
    "                    return 0 # integer 0   \n",
    "            sdc_lst = []\n",
    "            for focal_year in range(s_year-1, e_year):  \n",
    "                with open(tmp_data_path+f\"/a5_top_10_peers_tnic2_{focal_year}.pickle\", 'rb') as f:\n",
    "                    top_peers = pickle.load(f)\n",
    "                \n",
    "                \n",
    "\n",
    "                top_peers = top_peers[focal_gvkey] # a dataframe\n",
    "        #           print(top_peers)\n",
    "                top_peers_lst = top_peers.gvkey2.tolist()\n",
    "                selected_sdc_tnic = sdc_tnic[(sdc_tnic['AGVKEY'].isin(top_peers_lst + [focal_gvkey])) & (sdc_tnic.YEAR == focal_year+1) ] \n",
    "                selected_sdc_tnic.reset_index(drop=True)\n",
    "\n",
    "                if selected_sdc_tnic.shape[0] > 0:\n",
    "                    #print(selected_sdc_tnic[['DE', 'AGVKEY']] , top_peers[['gvkey2', 'score']])\n",
    "                    df = selected_sdc_tnic[['DA', 'AGVKEY', 'TGVKEY']]\n",
    "\n",
    "\n",
    "                    df['EVENT_TYPE'] = df.apply(helper1, axis=1)\n",
    "                    #print(df)\n",
    "\n",
    "                    score_df = top_peers[['gvkey2', 'score']]\n",
    "\n",
    "                    df = df.merge(score_df, left_on='AGVKEY', right_on = 'gvkey2', how = 'left')\n",
    "                    df = df[['DA','AGVKEY', 'EVENT_TYPE', 'score', 'TGVKEY']]\n",
    "    #                print(df)\n",
    "                    df = df.fillna(1)\n",
    "                    df.columns = ['UPDATE_DATE','AGVKEY','EVENT_TYPE', 'SCORE', 'TGVKEY'] # rename\n",
    "                    df = df.reset_index(drop=True)\n",
    "                    sdc_lst.append(df)\n",
    "                    #print(len(sdc_lst))\n",
    "            #print(focal_gvkey, len(sdc_lst))\n",
    "            assert len(sdc_lst) != 0, \"The Training Data Length is too short that some frequent acquirers have no event happened in that period\"\n",
    "            focal_df = pd.concat(sdc_lst, axis=0) \n",
    "            focal_c = focal_df.reset_index(drop=True) \n",
    "            focal_c = focal_c.sort_values(by = ['UPDATE_DATE']) # date time is unsortable..\n",
    "            focal_c.reset_index(drop=True, inplace=True)\n",
    "            return focal_c\n",
    "\n",
    "        def convert_date(df):\n",
    "            def datetime_converter(date_time):\n",
    "                base_time = np.datetime64('1997-01-01')\n",
    "                days_diff = np.datetime64(date_time.date()) - base_time\n",
    "                return days_diff.astype(int)\n",
    "            for idx, row in df.iterrows():\n",
    "                df.loc[idx, 'UPDATE_DATE'] = datetime_converter(df.loc[idx, 'UPDATE_DATE'])\n",
    "\n",
    "            df.sort_values(by = ['UPDATE_DATE']).reset_index(drop=True, inplace=True)\n",
    "            return df\n",
    "\n",
    "        def making_time_diff(focal_c2):\n",
    "            '''\n",
    "            df = focal_c; update date is the integer form that count the date from base_date (1997 01 01)\n",
    "\n",
    "            WARNING: the No.1 event set time-diff = 0\n",
    "            '''\n",
    "            tmp_columns = focal_c2.columns.tolist()\n",
    "      \n",
    "            focal_c2['UPDATE_DATE'] = [0] + [1 if timediff==0 else timediff for timediff in focal_c2.UPDATE_DATE.diff().tolist()[1:] ]\n",
    "            focal_c2.columns = ['time_diff'] + tmp_columns[1:]\n",
    "            return focal_c2\n",
    "\n",
    "        def __main__():\n",
    "            focal_c = get_focal_df(focal_gvkey)\n",
    "            focal_c2 = convert_date(focal_c.copy())\n",
    "            focal_c3 = making_time_diff(focal_c2.copy())\n",
    "            arr_c = np.array(focal_c3[['time_diff', 'EVENT_TYPE', 'SCORE']])\n",
    "            \n",
    "            return arr_c, focal_c\n",
    "\n",
    "        arr, focal_c = __main__()\n",
    "\n",
    "        return arr, focal_c\n",
    "\n",
    "    def get_arr_b(focal_c, focal_gvkey):\n",
    "        def add_datetime(df):\n",
    "            def helper(row):\n",
    "                return np.datetime64(str(row.year+1)+'-01-01')\n",
    "            df['UPDATE_DATE'] = df.apply(helper, axis=1)\n",
    "            return df\n",
    "        def obtain_fv(focal_gvkey, focal_c, fv):\n",
    "            ## here I +1 since I want to use the latest fin var  of that year\n",
    "            year_min, year_max = min([date.year for date in focal_c.UPDATE_DATE.tolist()]), max([date.year for date in focal_c.UPDATE_DATE.tolist()]) + 1 \n",
    "            fv_subset = fv[(fv.year >= year_min-1) & (fv.year <= year_max-1) & (fv.gvkey == focal_gvkey)]\n",
    "            fv_subset = fv_subset[['gvkey', 'year','UPDATE_DATE', 'at', 'sale', 'ch', 'm2b', 'lev', 'roa', 'ppe',\n",
    "               'cash2asset', 'cash2sale', 'sale2asset', 'de', 'roe', 'd_sale', 'd_at']]\n",
    "            fv_subset.columns=['AGVKEY', 'year','UPDATE_DATE', 'at', 'sale', 'ch', 'm2b', 'lev', 'roa', 'ppe',\n",
    "               'cash2asset', 'cash2sale', 'sale2asset', 'de', 'roe', 'd_sale', 'd_at']\n",
    "            \n",
    "            \n",
    "            return fv_subset\n",
    "\n",
    "        def __main__():\n",
    "            with open(tmp_data_path+\"/afreq_full_fv.pickle\", \"rb\") as f:\n",
    "                fv = pickle.load(f)\n",
    "            fv = add_datetime(fv)\n",
    "            focal_b = obtain_fv(\"5047\", focal_c, fv)\n",
    "            arr_b = np.array(focal_b.iloc[:, 3:])\n",
    "            arr_b = minmax_normalize(arr_b)\n",
    "            return arr_b, focal_b\n",
    "\n",
    "        arr = __main__()\n",
    "        return arr\n",
    "\n",
    "    def create_main_timeline(focal_b, focal_c):\n",
    "        '''\n",
    "        WARNING: GLOBAL and LOCAL time both start from 0!\n",
    "\n",
    "        '''\n",
    "        def helper(row):\n",
    "            if (row.EVENT_TYPE == 1) or (row.EVENT_TYPE == 0):\n",
    "                return 'past'\n",
    "            else:\n",
    "                return \"fv\"\n",
    "\n",
    "        def helper2(row):\n",
    "            if row.EVENT_TYPE == 1:\n",
    "                return \"1\"\n",
    "            elif row.EVENT_TYPE == 0:\n",
    "                return \"2\"\n",
    "            else:\n",
    "                return \"3\"\n",
    "\n",
    "        tmp = pd.concat([focal_c, focal_b]).sort_values(by=['UPDATE_DATE'])\n",
    "        tmp['EVENT_TYPE_countcreater'] = tmp.apply(helper, axis=1)\n",
    "        tmp['EVENT_TYPE_true'] = tmp.apply(helper2, axis=1)\n",
    "        tmp['LOCAL_IDX'] = tmp.groupby(['EVENT_TYPE_countcreater'])['UPDATE_DATE'].rank(ascending=True) -1 # rank start with 1\n",
    "        tmp['LOCAL_IDX'] = tmp['LOCAL_IDX'].astype(int)\n",
    "        ## with local_idx using rank, local idx is not continuous (may have gap)\n",
    "        \n",
    "        tmp_columns = tmp.columns\n",
    "        tmp.reset_index(drop=True, inplace=True)\n",
    "        tmp.reset_index(drop=False, inplace=True)\n",
    "\n",
    "        tmp.columns = ['GLOBAL_IDX']+ tmp_columns.tolist() # rename global index\n",
    "\n",
    "        tmp = tmp[['GLOBAL_IDX', 'LOCAL_IDX', 'UPDATE_DATE', 'EVENT_TYPE_true', 'TGVKEY']]\n",
    "\n",
    "        tmp.columns = ['GLOBAL_IDX', 'LOCAL_IDX', 'UPDATE_DATE', 'EVENT_TYPE', 'TGVKEY'] # rename\n",
    "\n",
    "\n",
    "\n",
    "        return tmp\n",
    "    \n",
    "    def __main__():\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            arr_c, focal_c = get_arr_c(focal_gvkey)\n",
    "        arr_b, focal_b = get_arr_b(focal_c, focal_gvkey)\n",
    "        timeline = create_main_timeline(focal_b, focal_c)\n",
    "        return arr_c, arr_b, timeline\n",
    "    \n",
    "    arr_c, arr_b, timeline = __main__()\n",
    "    return [arr_c, arr_b, timeline]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################# get delta time before\n",
    "\n",
    "def get_delta_time_previous(timelines, training_e_year):\n",
    "    '''\n",
    "    this function specific for:\n",
    "    - training end year \n",
    "                if training end year = 2018, then the date end is 2019-01-01\n",
    "\n",
    "    \n",
    "    output: lst, each element is a scalar \n",
    "    '''\n",
    "    previous_delta_times = []\n",
    "    for timeline in timelines:\n",
    "        last_time = timeline[timeline.EVENT_TYPE.isin(['1', '2'])].iloc[-1, 2]\n",
    "        time_diff = np.datetime64(f'{training_e_year+1}-01-01') - last_time\n",
    "        previous_delta_times.append(time_diff.days)\n",
    "    return previous_delta_times # of all frequent acquirers\n",
    "\n",
    "#################### get label (scalar)\n",
    "# 1: happened in the following year\n",
    "# 0: did not happened in the following year\n",
    "\n",
    "\n",
    "\n",
    "def get_label(training_e_year, forward = 1):\n",
    "    '''\n",
    "    if training_e_year = 2018, means \"if the focal firm triggered an MA event in 2019?\"\n",
    "    forward: how many years foward could next self event located in\n",
    "    '''\n",
    "    with open(data_path+f\"/dataset_top10_freq5_1997_2020.pickle\", \"rb\") as f:\n",
    "        _, _, timelines = pickle.load(f) \n",
    "\n",
    "    # load full data\n",
    "    labels = []\n",
    "    for timeline in timelines:\n",
    "        year_after = timeline[(timeline.UPDATE_DATE > np.datetime64(f'{training_e_year+1}-01-01')) & (timeline.UPDATE_DATE < np.datetime64(f'{training_e_year+1+forward}-01-01')) &  (timeline.EVENT_TYPE == '1')]\n",
    "        if year_after.shape[0] == 0:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################### \n",
    "\n",
    "\n",
    "######################################## from  variables #######################\n",
    "'''\n",
    "the input is arr_cs, arr_bs, timelines, fv, tnic \n",
    "the output is the data described in model.py\n",
    "\n",
    "'''\n",
    "# creating helpers\n",
    "def get_arr_b_idx(df):\n",
    "    '''\n",
    "    df is the a timeline table of a single firm (the 3rd output of preprocess function)\n",
    "    output is a list\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    sample_df = df.copy()\n",
    "    sub1 = sample_df[(sample_df.EVENT_TYPE == '1')& (sample_df.LOCAL_IDX >2)] # happened at 3rd time \n",
    "\n",
    "    global_idxs = sub1.GLOBAL_IDX.values # array\n",
    "\n",
    "    arr_b_idxs = []\n",
    "    for global_idx in global_idxs:\n",
    "        sub2 = sample_df[(sample_df.EVENT_TYPE == '3') & (sample_df.GLOBAL_IDX < global_idx)]\n",
    "        arr_b_idx = sub2.iloc[-1, 1]\n",
    "        arr_b_idxs.append(arr_b_idx)\n",
    "\n",
    "    return arr_b_idxs\n",
    "\n",
    "def get_c_t_idx(df):\n",
    "    '''\n",
    "    df: timeline\n",
    "    '''\n",
    "    sample_df = df.copy()\n",
    "    sub1 = sample_df[(sample_df.EVENT_TYPE == '1')& (sample_df.LOCAL_IDX >2)] # why > 2? since we at least need 2 warmup time for c \n",
    "    local_idxs = sub1.LOCAL_IDX.values # array\n",
    "    \n",
    "    arr_c_idxs = []\n",
    "    arr_t_idxs = []\n",
    "    for local_idx in local_idxs:\n",
    "        sub2 = sample_df[(sample_df.EVENT_TYPE.isin(['1','2'])) & (sample_df.LOCAL_IDX < local_idx)]\n",
    "        \n",
    "        # arr_c_idx and arr_t_idx is 1 row dif\n",
    "        arr_c_idx = sub2.iloc[-1, 1]  # why? since the delta_t input of c is the time difference between (event-) and (event- -)\n",
    "        arr_t_idx = sub2.iloc[-1, 1]\n",
    "        arr_c_idxs.append(arr_c_idx)\n",
    "        arr_t_idxs.append(arr_t_idx)\n",
    "    return arr_c_idxs, arr_t_idxs\n",
    "\n",
    "def convert_date(df):\n",
    "    df = df.copy()\n",
    "    def datetime_converter(date_time):\n",
    "        base_time = np.datetime64('1997-01-01')\n",
    "        days_diff = np.datetime64(date_time.date()) - base_time\n",
    "        return days_diff.astype(int)\n",
    "    for idx, row in df.iterrows():\n",
    "        df.loc[idx, 'UPDATE_DATE_int'] = datetime_converter(df.loc[idx, 'UPDATE_DATE'])\n",
    "\n",
    "    #df.sort_values(by = ['UPDATE_DATE']).reset_index(drop=True, inplace=True)\n",
    "    return df \n",
    "\n",
    "def sample_negative_time_point(df, base_n_sample=10):\n",
    "    '''\n",
    "    df is timeline, but + 'UPDATE_DATE_int'\n",
    "    \n",
    "    number of negative samples is corresponding to the number of positive samples (follow the idea of negative smapling in skip-gram)\n",
    "        each word, approx 10 negative samples.\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    max_time = df.UPDATE_DATE_int.values[-1] ### \n",
    "    sub_df = df[df.EVENT_TYPE.isin(['1', '2']) & (df.LOCAL_IDX >2)]\n",
    "    min_time = sub_df.UPDATE_DATE_int.values[0]\n",
    "    n_event = df[(df.EVENT_TYPE == '1') & (df.LOCAL_IDX >2)].shape[0]\n",
    "    if n_event == 0:\n",
    "        n_event = 1\n",
    "    n_samples = base_n_sample * n_event\n",
    "    samples = np.random.uniform(low=min_time, high=max_time, size=n_samples)\n",
    "    return samples, max_time - min_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_arr_b_idx_neg(time_samples, df):\n",
    "    '''\n",
    "    df is timeline + 'UPDATE_DATE_int'\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df_b = df[df.EVENT_TYPE == '3']\n",
    "    arr_b_idxs = []\n",
    "    for time in time_samples:\n",
    "        df_b_sub = df_b[df_b.UPDATE_DATE_int<time]\n",
    "        arr_b_idxs.append(df_b_sub.iloc[-1, 1])\n",
    "    \n",
    "    return arr_b_idxs\n",
    "\n",
    "\n",
    "\n",
    "def get_arr_c_t_idx_neg(time_samples, df):\n",
    "    '''\n",
    "    df is timeline + 'UPDATE_DATE_int'\n",
    "    total columns are: [GLOBAL_IDX  LOCAL_IDX UPDATE_DATE EVENT_TYPE  UPDATE_DATE_int]\n",
    "\n",
    "    time_samples, list of integers\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df_c = df[df.EVENT_TYPE.isin(['1', '2']) & (df.LOCAL_IDX >2)]\n",
    "\n",
    "    arr_c_idxs_neg = []\n",
    "    arr_t_neg = []\n",
    "    for time in time_samples:\n",
    "        df_before = df_c[df_c.UPDATE_DATE_int < time]\n",
    "        #print(time)\n",
    "        \n",
    "        arr_c_idx_neg = df_before.iloc[-1, 1] # here do not -1!\n",
    "        previous_time = df_before.iloc[-1, 5]\n",
    "        \n",
    "        arr_c_idxs_neg.append(arr_c_idx_neg)\n",
    "        #print(time, previous_time)\n",
    "        arr_t_neg.append(time - previous_time)\n",
    "    \n",
    "    return arr_c_idxs_neg, np.array(arr_t_neg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_arr_b_c_idx_i(df, s_year, e_year):\n",
    "    '''\n",
    "    df is timeline + 'UPDATE_DATE_int'\n",
    "    total columns are: [GLOBAL_IDX  LOCAL_IDX UPDATE_DATE EVENT_TYPE  UPDATE_DATE_int]\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    # create a year variable\n",
    "    def helper(row):\n",
    "        return row.UPDATE_DATE.year\n",
    "    df['year'] = df.apply(helper, axis=1)\n",
    "    \n",
    "    # qualified self event\n",
    "    sub = df[(df.EVENT_TYPE == '1') & (df.LOCAL_IDX > 2)]\n",
    "    \n",
    "    yearly = {}\n",
    "    for year in range(s_year, e_year+1):\n",
    "        b_idxs = []\n",
    "        c_idxs = []\n",
    "        sub2 = sub[sub.year == year] # self event at particular year\n",
    "        for _, row in sub2.iterrows():\n",
    "            time = row.UPDATE_DATE_int # float\n",
    "            # back to global df\n",
    "            df_b_before = df[(df.UPDATE_DATE_int < time)&(df.EVENT_TYPE == '3')]\n",
    "            df_c_before = df[(df.UPDATE_DATE_int < time)&(df.EVENT_TYPE.isin(['1','2']))]\n",
    "            idx_b = df_b_before.iloc[-1, 1]\n",
    "            idx_c = df_c_before.iloc[-1, 1] -1\n",
    "            b_idxs.append(idx_b)\n",
    "            c_idxs.append(idx_c)\n",
    "            \n",
    "        yearly[year] = (np.array(b_idxs), np.array(c_idxs))\n",
    "    \n",
    "    return yearly\n",
    "        \n",
    "\n",
    "def true_tar_idxs_i(timeline, dict_idx):\n",
    "    '''\n",
    "    year loop by self-event year\n",
    "        TNIC related data use year-1\n",
    "    \n",
    "    '''\n",
    "\n",
    "        \n",
    "    # add year to timeline data\n",
    "    df = timeline.copy()\n",
    "    # create a year variable\n",
    "    def helper(row):\n",
    "        return row.UPDATE_DATE.year\n",
    "    df['year'] = df.apply(helper, axis=1)\n",
    "    \n",
    "    # qualified self event\n",
    "    sub = df[(df.EVENT_TYPE == '1') & (df.LOCAL_IDX > 2)]\n",
    "    \n",
    "    yearly = {}\n",
    "    # loop over self-merge year\n",
    "    for year in range(s_year, e_year+1):\n",
    "        '''\n",
    "        N_i_1 = num of candidate target\n",
    "        N_i_2 = num of self event\n",
    "        '''\n",
    "        N_i_1 = len(gvkey_lsts[year-1]) # all target candidate in TNIC net\n",
    "        b_idxs, c_idxs = dict_idx[year] # the output of ...\n",
    "        N_i_2 = len(b_idxs)\n",
    "        timeline_i = sub[sub.year == year] # only ith year\n",
    "        targets_lst = timeline_i.TGVKEY.values.tolist() # length = N_i_2\n",
    "        assert len(targets_lst) == N_i_2, \"length dismatch with larget lists and N_i_2\"\n",
    "        idx_lst = [key_ind_maps[year-1][tgvkey] for tgvkey in targets_lst]\n",
    "        #one_hot_i = (np.arange(_ == a[...,None]-1).astype(int)\n",
    "        a = np.array(idx_lst)\n",
    "        one_hot_i = (np.arange(N_i_1) == a[...,None]).astype(int)\n",
    "        yearly[year] = one_hot_i\n",
    "    return yearly\n",
    "\n",
    "\n",
    "\n",
    "def get_node_features(fv_full, gvkey_lsts, key_ind_maps , ind_key_maps, s_year=1997, e_year=2020):\n",
    "    '''\n",
    "    fv_full: raw\n",
    "    gvkey_lsts, key_ind_maps , ind_key_maps: raw\n",
    "    \n",
    "    WARNING: the output yearly's year is self-event's year!!! \n",
    "    '''\n",
    "    # loop self-merge year\n",
    "    yearly = {}\n",
    "    for year in range(s_year, e_year+1): \n",
    "        df_gvkeys = pd.DataFrame({'gvkeys': gvkey_lsts[year-1]})\n",
    "        fv_candidate = fv_full[fv_full.gvkey.isin(gvkey_lsts[year-1]) & (fv_full.year == year-1)]\n",
    "        fv_i = df_gvkeys.merge(fv_candidate, left_on='gvkeys', right_on = 'gvkey', how = \"left\")\n",
    "        fv_i.reset_index(drop=True, inplace=True)\n",
    "        #print(fv_i[:5])\n",
    "        arr = fv_i.iloc[:, 3:].to_numpy()\n",
    "        yearly[year] = arr\n",
    "        assert len(gvkey_lsts[year-1]) == arr.shape[0], \"list and arr shape dismatch\"\n",
    "    \n",
    "    return yearly\n",
    "\n",
    "def get_net_structure(tmp_data_path, gvkey_lsts, key_ind_maps , ind_key_maps, s_year=1997, e_year=2020):\n",
    "    \n",
    "    yearly = {}    \n",
    "    # loop over self-event year! not TNIC !\n",
    "    for year in range(s_year, e_year+1):     \n",
    "        with open(tmp_data_path+f'/a5_top_10_peers_tnic2_{year-1}.pickle', 'rb') as f:\n",
    "            tnic = pickle.load(f)   \n",
    "            df_all_lst = []\n",
    "            for _,value in tnic.items():\n",
    "                df_all_lst.append(value)\n",
    "            df_all = pd.concat(df_all_lst)\n",
    "            df_net = df_all[['gvkey1', 'gvkey2']]\n",
    "            lst1 = df_net.gvkey1.values.tolist()\n",
    "            lst2 = df_net.gvkey2.values.tolist()\n",
    "            idx1 = [key_ind_maps[year-1][gvkey1] for gvkey1 in lst1]\n",
    "            idx2 = [key_ind_maps[year-1][gvkey2] for gvkey2 in lst2]\n",
    "            arr = np.array([idx1, idx2])\n",
    "            assert arr.shape[0] == 2, \"the dim of output is wrong\"\n",
    "        yearly[year] = arr\n",
    "    return yearly\n",
    "\n",
    "\n",
    "######## main helper ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_step1_data(training_e_year, load_timeline_from_pickle, forward=1, multi_tasking=False):\n",
    "    '''\n",
    "    step1 data includes: arr_cs, arr_bs, timelines, previous_delta_time, labels\n",
    "    \n",
    "    '''\n",
    "    ########### creating  arr_cs, arr_bs, timelines\n",
    "    if load_timeline_from_pickle:\n",
    "        with open(data_path+f\"/dataset_top10_freq5_{training_s_year}_{training_e_year}.pickle\", \"rb\") as f:\n",
    "            arr_cs, arr_bs, timelines = pickle.load(f)   \n",
    "\n",
    "    else:\n",
    "\n",
    "        arr_cs = []\n",
    "        arr_bs = []\n",
    "        timelines = []\n",
    "        idx_to_gvkey = {}\n",
    "        results = []\n",
    "        gvkey_to_idx= {}\n",
    "        \n",
    "\n",
    "        print(\"##### creating step 1 data #####\")\n",
    "        if multi_tasking:\n",
    "            for i, gvkey in enumerate(a_freq_lst):\n",
    "                results.append(dataloader_preproceser(gvkey))\n",
    "                idx_to_gvkey[i] = gvkey\n",
    "\n",
    "            with ProgressBar():\n",
    "                results = dask.compute(results, num_workers = os.cpu_count())\n",
    "            results = results[0]\n",
    "            arr_cs, arr_bs, timelines = [ele[0] for ele in results], [ele[1] for ele in results], [ele[2] for ele in results]\n",
    "        else:\n",
    "            for i, gvkey in enumerate(tqdm(a_freq_lst[:2])):\n",
    "                arr_c, arr_b, timeline = dataloader_preproceser(gvkey)\n",
    "                arr_cs.append(arr_c)\n",
    "                arr_bs.append(arr_b)\n",
    "                timelines.append(timeline)\n",
    "                idx_to_gvkey[i] = gvkey\n",
    "                gvkey_to_idx[gvkey] = i\n",
    "\n",
    "            \n",
    "        #assert len(arr_cs) == len(arr_bs) == len(timelines) == len(a_freq_lst), \"length of 3 outputs dismatch with len(a_freq_lst)\"\n",
    "\n",
    "        with open(data_path+f\"/dataset_top10_freq5_test_{training_s_year}_{training_e_year}.pickle\", \"wb\") as f:\n",
    "            pickle.dump((arr_cs, arr_bs, timelines), f)\n",
    "\n",
    "    ############ creating  previous_delta_time, labels\n",
    "    previous_delta_time = get_delta_time_previous(timelines, training_e_year)\n",
    "    labels = get_label(training_e_year, forward=forward)\n",
    "    \n",
    "    return arr_cs, arr_bs, timelines, np.array(previous_delta_time), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(arr_cs, arr_bs, timelines, previous_delta_time, labels):\n",
    "    ma_dataset = []\n",
    "    print(\"##### creating step 2 data #####\")\n",
    "    for i, gvkey in enumerate(tqdm(a_freq_lst[:2])):\n",
    "        '''\n",
    "        the rest variables are for a specific freq acquirer\n",
    "        \n",
    "        Output:\n",
    "            ma_dataset is a list\n",
    "                all elements are np-arrays\n",
    "        '''\n",
    "        \n",
    "        \n",
    "    \n",
    "        ######### get arr_c and arr_delta_time\n",
    "        arr_c = arr_cs[i]\n",
    "        #arr_c = arr_c[1:] # remove the first row! Since there's no \"delta_t\" data for the first self/peer event\n",
    "        arr_delta_time = arr_c[1:, 0] \n",
    "        # get arr_b\n",
    "        arr_b = arr_bs[i]\n",
    "        \n",
    "        ######## Event data\n",
    "        timeline = convert_date(timelines[i]) # add UPDATE_DATE_int column\n",
    "        #print(timeline.columns)\n",
    "        # arr_b_idx\n",
    "        arr_b_idx = get_arr_b_idx(timeline)\n",
    "        arr_b_idx = np.array(arr_b_idx)\n",
    "        # arr_c_idx and arr_delta_time\n",
    "        arr_c_idx, _ = get_c_t_idx(timeline)\n",
    "\n",
    "        # arr_t_idx is the same as arr_c_idx\n",
    "        arr_c_idx = np.array(arr_c_idx)\n",
    "        arr_t_idx = arr_c_idx\n",
    "\n",
    "        arr_delta_time = arr_delta_time[list(arr_t_idx)]\n",
    "\n",
    "        event_data = (arr_b_idx, arr_c_idx, arr_delta_time) #############\n",
    "        \n",
    "        ######## Non- Event data\n",
    "        # estimate_time_length\n",
    "        samples, estimate_time_length = sample_negative_time_point(timeline)\n",
    "        #  arr_b_idx\n",
    "        non_arr_b_idx =  get_arr_b_idx_neg(samples, timeline)\n",
    "        non_arr_b_idx = np.array(non_arr_b_idx)\n",
    "\n",
    "        non_arr_c_idx, non_arr_delta_time = get_arr_c_t_idx_neg(samples, timeline)\n",
    "        non_arr_c_idx = np.array(non_arr_c_idx)\n",
    "\n",
    "        non_event_data = (non_arr_b_idx, non_arr_c_idx, non_arr_delta_time) #############\n",
    "        ######## Choice data dict\n",
    "        dict_idx = get_arr_b_c_idx_i(timeline, s_year, e_year)\n",
    "        true_tar_idxs = true_tar_idxs_i(timeline, dict_idx)\n",
    "        node_feature = get_node_features(fv_full, gvkey_lsts, key_ind_maps , ind_key_maps, s_year, e_year)\n",
    "        net_structure = get_net_structure(tmp_data_path, gvkey_lsts, key_ind_maps , ind_key_maps, s_year, e_year)\n",
    "        choice_data_dict = (dict_idx, true_tar_idxs, node_feature, net_structure)\n",
    "        ma_dataset.append((arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_time_length, choice_data_dict, previous_delta_time, labels))\n",
    "  \n",
    "    return ma_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ Global Variables\n",
    "\n",
    "tmp_data_path = '../MA_data/data/tmp'\n",
    "data_path = '../MA_data/data'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## loading pre-prepared dataset\n",
    "# YEAR invariant\n",
    "# WARNING: self event at year \"y\" will use tnic and fv data at year \"y-1\"\n",
    "sdc_tnic = pd.read_pickle(tmp_data_path+f\"/sdc_tnic_1997_2020\") # always load full dataset\n",
    "sdc_tnic = same_day_only_one(sdc_tnic) \n",
    "\n",
    "# TNIC structure\n",
    "with open(tmp_data_path+f\"/tnic_info_3_pairs_{1997-1}_{2020-1}\", 'rb') as f: # read full\n",
    "    gvkey_lsts, key_ind_maps , ind_key_maps = pickle.load(f)\n",
    "\n",
    "# frequent acquirer \n",
    "with open(data_path+f\"/freq_a_info_1997_2020.pickle\", \"rb\") as f: \n",
    "    A_freq, a_freq_lst, a_freq_idx_to_gvkey_mapping, a_freq_gvkey_to_idx_mapping = pickle.load(f)\n",
    "\n",
    "\n",
    "# load full fv raw and normalize it\n",
    "with open(data_path+\"/fv_raw_full_1996_2019.pickle\", 'rb') as f:\n",
    "    fv_full = pickle.load(f)\n",
    "fv_full = pd.concat([fv_full.iloc[:,:2], minmax_normalize(fv_full.iloc[:, 2:])], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "############\n",
    "load_timeline_from_pickle = False\n",
    "\n",
    "############ creating, year variant\n",
    "training_s_year = 1997\n",
    "s_year = training_s_year\n",
    "#training_e_year = 2018 # fitting data til y-12-31\n",
    "\n",
    "\n",
    "for training_e_year  in tqdm([2018]):\n",
    "    e_year = training_e_year\n",
    "    testing_s_year = training_e_year\n",
    "    testing_e_year = testing_s_year+1\n",
    "    forward= 1\n",
    "\n",
    "        \n",
    "    sdc_tnic = sdc_tnic[(sdc_tnic.YEAR <= training_e_year)] \n",
    "\n",
    "\n",
    "    # create arr_cs, arr_bs, timelines, previous_delta_time, labels\n",
    "    arr_cs, arr_bs, timelines, previous_delta_time, labels = get_step1_data(training_e_year, load_timeline_from_pickle, forward=forward)\n",
    "    ma_dataset = create_dataset(arr_cs, arr_bs, timelines, previous_delta_time, labels)\n",
    "    with open(data_path+f\"/ma_dataset_test_{training_e_year}_{forward}.pickle\", 'wb') as f:\n",
    "        pickle.dump(ma_dataset, f)\n",
    "\n",
    "\n",
    "\n",
    "############################# so far, we have arr_cs, arr_bs, timelines, previous_delta_time, labels #############\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check data\n",
    "\n",
    "1. check shape\n",
    "2. check value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:13:25.098298Z",
     "start_time": "2021-11-03T16:13:25.094256Z"
    }
   },
   "outputs": [],
   "source": [
    "non_arr_b_idx, non_arr_c_idx, non_arr_delta_time = non_event_data\n",
    "arr_b_idx,arr_c_idx,arr_delta_time = event_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:13:33.968209Z",
     "start_time": "2021-11-03T16:13:33.962096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_arr_b_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:13:42.356990Z",
     "start_time": "2021-11-03T16:13:42.350945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_arr_c_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:13:51.874824Z",
     "start_time": "2021-11-03T16:13:51.868763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_arr_delta_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:13:59.108222Z",
     "start_time": "2021-11-03T16:13:59.102695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_arr_b_idx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:14:07.105787Z",
     "start_time": "2021-11-03T16:14:07.100070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:14:15.014322Z",
     "start_time": "2021-11-03T16:14:15.007834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_arr_c_idx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:14:19.622343Z",
     "start_time": "2021-11-03T16:14:19.616148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:12:24.583445Z",
     "start_time": "2021-11-03T16:12:24.576562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 1., 1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_c[arr_c_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:10:51.185764Z",
     "start_time": "2021-11-03T16:10:51.179449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_b_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T16:10:45.094565Z",
     "start_time": "2021-11-03T16:10:45.088597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_delta_time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:23:00.430590Z",
     "start_time": "2021-11-03T17:23:00.425074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([209,  32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_delta_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
