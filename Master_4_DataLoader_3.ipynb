{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.037847Z",
     "start_time": "2021-10-29T15:36:08.033492Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each firm, you need to have corresponding:\n",
    "\n",
    "[arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_length, choice_data_dict]\n",
    "\n",
    "\n",
    "in the Dataset object, at least should have such things\n",
    "- idx_to_gvkey mapping (idx max = 511)\n",
    "    - every time you call an idx, first transfer it to gvkey, and get a batch of \n",
    "    `[arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_length, choice_data_dict]`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.058892Z",
     "start_time": "2021-10-29T15:36:08.051745Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_data_path = '../MA_data/data/tmp'\n",
    "data_path = '../MA_data/data'\n",
    "\n",
    "s_year = 1997\n",
    "e_year = 2020\n",
    "\n",
    "load_timeline_from_pickle = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.109051Z",
     "start_time": "2021-10-29T15:36:08.077246Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataloader_preproceser(focal_gvkey):\n",
    "    \n",
    "    def get_arr_c(focal_gvkey):\n",
    "    # part 1, get c  \n",
    "        def get_focal_df(focal_gvkey):\n",
    "            '''\n",
    "            output: will be a df contains 3 columns: DATE, AGVKEY, EVENT_TYPE, SCORE\n",
    "                DATE: datetime.dt object\n",
    "                AGVKEY: str: 4 - 6 digits\n",
    "                EVENT_TYPE: 1:self 0:peer (integer)\n",
    "                SCORE: TNIC similarity last year for event type 0, otherwise 1\n",
    "            \n",
    "            Use DA!!!! not DE\n",
    "\n",
    "            '''\n",
    "            def helper1(row):\n",
    "                if row.AGVKEY == focal_gvkey:\n",
    "                    return 1 # integer 1\n",
    "                else:\n",
    "                    return 0 # integer 0   \n",
    "            sdc_lst = []\n",
    "            for focal_year in range(s_year-1, e_year):  \n",
    "                with open(tmp_data_path+f\"/a5_top_10_peers_tnic2_{focal_year}.pickle\", 'rb') as f:\n",
    "                    top_peers = pickle.load(f)\n",
    "                try:\n",
    "                    top_peers = top_peers[focal_gvkey] # a dataframe\n",
    "         #           print(top_peers)\n",
    "                    top_peers_lst = top_peers.gvkey2.tolist()\n",
    "                    selected_sdc_tnic = sdc_tnic[ (sdc_tnic['AGVKEY'].isin(top_peers_lst + [focal_gvkey])) & (sdc_tnic.YEAR == focal_year+1) ] \n",
    "                    selected_sdc_tnic.reset_index(drop=True)\n",
    "                    if selected_sdc_tnic.shape[0] > 0:\n",
    "                        #print(selected_sdc_tnic[['DE', 'AGVKEY']] , top_peers[['gvkey2', 'score']])\n",
    "                        df = selected_sdc_tnic[['DA', 'AGVKEY', 'TGVKEY']]\n",
    "\n",
    "\n",
    "                        df['EVENT_TYPE'] = df.apply(helper1, axis=1)\n",
    "                        #print(df)\n",
    "\n",
    "                        score_df = top_peers[['gvkey2', 'score']]\n",
    "\n",
    "                        df = df.merge(score_df, left_on='AGVKEY', right_on = 'gvkey2', how = 'left')\n",
    "                        df = df[['DA','AGVKEY', 'EVENT_TYPE', 'score', 'TGVKEY']]\n",
    "        #                print(df)\n",
    "                        df = df.fillna(1)\n",
    "                        df.columns = ['UPDATE_DATE','AGVKEY','EVENT_TYPE', 'SCORE', 'TGVKEY'] # rename\n",
    "                        df = df.reset_index(drop=True)\n",
    "                        sdc_lst.append(df)\n",
    "                    #print(len(sdc_lst))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            focal_df = pd.concat(sdc_lst, axis=0)\n",
    "            focal_c = focal_df.reset_index(drop=True) \n",
    "            focal_c = focal_c.sort_values(by = ['UPDATE_DATE']) # date time is unsortable..\n",
    "            focal_c.reset_index(drop=True, inplace=True)\n",
    "            return focal_c\n",
    "\n",
    "        def convert_date(df):\n",
    "            def datetime_converter(date_time):\n",
    "                base_time = np.datetime64('1997-01-01')\n",
    "                days_diff = np.datetime64(date_time.date()) - base_time\n",
    "                return days_diff.astype(int)\n",
    "            for idx, row in df.iterrows():\n",
    "                df.loc[idx, 'UPDATE_DATE'] = datetime_converter(df.loc[idx, 'UPDATE_DATE'])\n",
    "\n",
    "            df.sort_values(by = ['UPDATE_DATE']).reset_index(drop=True, inplace=True)\n",
    "            return df\n",
    "\n",
    "        def making_time_diff(focal_c2):\n",
    "            '''\n",
    "            df = focal_c; update date is the integer form that count the date from base_date (1997 01 01)\n",
    "\n",
    "            WARNING: the No.1 event set time-diff = 0\n",
    "            '''\n",
    "            tmp_columns = focal_c2.columns.tolist()\n",
    "            focal_c2['UPDATE_DATE'] = [0] + [1 if timediff==0 else timediff for timediff in focal_c2.UPDATE_DATE.diff().tolist()[1:] ]\n",
    "            focal_c2.columns = ['time_diff'] + tmp_columns[1:]\n",
    "            return focal_c2\n",
    "\n",
    "        def __main__():\n",
    "            focal_c = get_focal_df(focal_gvkey)\n",
    "            focal_c2 = convert_date(focal_c.copy())\n",
    "            focal_c3 = making_time_diff(focal_c2.copy())\n",
    "            arr_c = np.array(focal_c3[['time_diff', 'EVENT_TYPE', 'SCORE']])\n",
    "            return arr_c, focal_c\n",
    "\n",
    "        arr, focal_c = __main__()\n",
    "\n",
    "        return arr, focal_c\n",
    "\n",
    "    def get_arr_b(focal_c, focal_gvkey):\n",
    "        def add_datetime(df):\n",
    "            def helper(row):\n",
    "                return np.datetime64(str(row.year+1)+'-01-01')\n",
    "            df['UPDATE_DATE'] = df.apply(helper, axis=1)\n",
    "            return df\n",
    "        def obtain_fv(focal_gvkey, focal_c, fv):\n",
    "            year_min, year_max = min([date.year for date in focal_c.UPDATE_DATE.tolist()]), max([date.year for date in focal_c.UPDATE_DATE.tolist()])\n",
    "            fv_subset = fv[(fv.year >= year_min-1) & (fv.year <= year_max-1) & (fv.gvkey == focal_gvkey)]\n",
    "            fv_subset = fv_subset[['gvkey', 'year','UPDATE_DATE', 'at', 'sale', 'ch', 'm2b', 'lev', 'roa', 'ppe',\n",
    "               'cash2asset', 'cash2sale', 'sale2asset', 'de', 'roe', 'd_sale', 'd_at']]\n",
    "            fv_subset.columns=['AGVKEY', 'year','UPDATE_DATE', 'at', 'sale', 'ch', 'm2b', 'lev', 'roa', 'ppe',\n",
    "               'cash2asset', 'cash2sale', 'sale2asset', 'de', 'roe', 'd_sale', 'd_at']\n",
    "            return fv_subset\n",
    "\n",
    "        def __main__():\n",
    "            with open(tmp_data_path+\"/afreq_full_fv.pickle\", \"rb\") as f:\n",
    "                fv = pickle.load(f)\n",
    "            fv = add_datetime(fv)\n",
    "            focal_b = obtain_fv(\"5047\", focal_c, fv)\n",
    "            arr_b = np.array(focal_b.iloc[:, 3:])\n",
    "            return arr_b, focal_b\n",
    "\n",
    "        arr = __main__()\n",
    "        return arr\n",
    "\n",
    "    def create_main_timeline(focal_b, focal_c):\n",
    "        '''\n",
    "        WARNING: GLOBAL and LOCAL time both start from 0!\n",
    "\n",
    "        '''\n",
    "        def helper(row):\n",
    "            if (row.EVENT_TYPE == 1) or (row.EVENT_TYPE == 0):\n",
    "                return 'past'\n",
    "            else:\n",
    "                return \"fv\"\n",
    "\n",
    "        def helper2(row):\n",
    "            if row.EVENT_TYPE == 1:\n",
    "                return \"1\"\n",
    "            elif row.EVENT_TYPE == 0:\n",
    "                return \"2\"\n",
    "            else:\n",
    "                return \"3\"\n",
    "\n",
    "        tmp = pd.concat([focal_c, focal_b]).sort_values(by=['UPDATE_DATE'])\n",
    "        tmp['EVENT_TYPE_countcreater'] = tmp.apply(helper, axis=1)\n",
    "        tmp['EVENT_TYPE_true'] = tmp.apply(helper2, axis=1)\n",
    "        tmp['LOCAL_IDX'] = tmp.groupby(['EVENT_TYPE_countcreater'])['UPDATE_DATE'].rank(ascending=True) -1 # rank start with 1\n",
    "        tmp['LOCAL_IDX'] = tmp['LOCAL_IDX'].astype(int)\n",
    "        ## with local_idx using rank, local idx is not continuous (may have gap)\n",
    "        \n",
    "        tmp_columns = tmp.columns\n",
    "        tmp.reset_index(drop=True, inplace=True)\n",
    "        tmp.reset_index(drop=False, inplace=True)\n",
    "\n",
    "        tmp.columns = ['GLOBAL_IDX']+ tmp_columns.tolist() # rename global index\n",
    "\n",
    "        tmp = tmp[['GLOBAL_IDX', 'LOCAL_IDX', 'UPDATE_DATE', 'EVENT_TYPE_true', 'TGVKEY']]\n",
    "\n",
    "        tmp.columns = ['GLOBAL_IDX', 'LOCAL_IDX', 'UPDATE_DATE', 'EVENT_TYPE', 'TGVKEY'] # rename\n",
    "\n",
    "\n",
    "\n",
    "        return tmp\n",
    "    \n",
    "    def __main__():\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            arr_c, focal_c = get_arr_c(focal_gvkey)\n",
    "        arr_b, focal_b = get_arr_b(focal_c, focal_gvkey)\n",
    "        timeline = create_main_timeline(focal_b, focal_c)\n",
    "        return arr_c, arr_b, timeline\n",
    "    \n",
    "    arr_c, arr_b, timeline = __main__()\n",
    "    return arr_c, arr_b, timeline\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.117880Z",
     "start_time": "2021-10-29T15:36:08.110394Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_freq_a(sdc_tnic, min_event=5):\n",
    "    A_freq = pd.DataFrame(sdc_tnic.AGVKEY.value_counts()).reset_index(drop=False)\n",
    "    A_freq = A_freq[A_freq.AGVKEY >= min_event]\n",
    "    A_freq.columns = [\"GVKEY\", \"freq\"]\n",
    "    print(f\"totally {A_freq.shape[0]} numbers of frequent Acquirers\")\n",
    "    a_freq_idx_to_gvkey_mapping = {}\n",
    "    a_freq_gvkey_to_idx_mapping = {}\n",
    "    for i, row in A_freq.iterrows():\n",
    "        a_freq_idx_to_gvkey_mapping[i] = row.GVKEY\n",
    "        a_freq_gvkey_to_idx_mapping[row.GVKEY] = i\n",
    "    a_freq_lst = A_freq.GVKEY.values.tolist()\n",
    "    return A_freq,a_freq_lst, a_freq_idx_to_gvkey_mapping, a_freq_gvkey_to_idx_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.124514Z",
     "start_time": "2021-10-29T15:36:08.119664Z"
    }
   },
   "outputs": [],
   "source": [
    "def same_day_only_one(sdc_tnic_raw):\n",
    "    print(\"shape before removing same-day multi events:\", sdc_tnic_raw.shape)\n",
    "    sdc_tnic_raw = sdc_tnic_raw.copy()\n",
    "    sdc_tnic_one = sdc_tnic_raw.groupby(['AGVKEY', 'DA']).first().reset_index(drop=False)\n",
    "    print(\"shape after removing same-day multi events:\", sdc_tnic_one.shape)\n",
    "    sdc_tnic_one.sort_values(by = ['DA'], axis=0, inplace=True)\n",
    "    return sdc_tnic_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.261311Z",
     "start_time": "2021-10-29T15:36:08.126151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before removing same-day multi events: (9661, 50)\n",
      "shape after removing same-day multi events: (9448, 50)\n"
     ]
    }
   ],
   "source": [
    "sdc_tnic = pd.read_pickle(tmp_data_path+f\"/sdc_tnic_{s_year}_{e_year}\")\n",
    "sdc_tnic = same_day_only_one(sdc_tnic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.296450Z",
     "start_time": "2021-10-29T15:36:08.263029Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(tmp_data_path+f\"/tnic_info_3_pairs_{s_year-1}_{e_year-1}\", 'rb') as f:\n",
    "    gvkey_lsts, key_ind_maps , ind_key_maps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T15:36:08.332334Z",
     "start_time": "2021-10-29T15:36:08.298431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 496 numbers of frequent Acquirers\n"
     ]
    }
   ],
   "source": [
    "A_freq, a_freq_lst, a_freq_idx_to_gvkey_mapping, a_freq_gvkey_to_idx_mapping = create_freq_a(sdc_tnic)\n",
    "\n",
    "a_freq_info = (A_freq, a_freq_lst, a_freq_idx_to_gvkey_mapping, a_freq_gvkey_to_idx_mapping)\n",
    "\n",
    "with open(data_path+f\"/freq_a_info_{s_year}_{e_year}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(a_freq_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.041816Z",
     "start_time": "2021-10-29T15:36:08.333574Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [24:57<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "if load_timeline_from_pickle:\n",
    "    with open(data_path+f\"/dataset_top10_freq5_{s_year}_{e_year}.pickle\", \"rb\") as f:\n",
    "        arr_cs, arr_bs, timelines = pickle.load(f)\n",
    "else:\n",
    "    arr_cs = []\n",
    "    arr_bs = []\n",
    "    timelines = []\n",
    "    idx_to_gvkey = {}\n",
    "\n",
    "    for i, gvkey in enumerate(tqdm(a_freq_lst)):\n",
    "        arr_c, arr_b, timeline = dataloader_preproceser(gvkey)\n",
    "        \n",
    "        arr_cs.append(arr_c)\n",
    "        arr_bs.append(arr_b)\n",
    "        timelines.append(timeline)\n",
    "        \n",
    "        idx_to_gvkey[i] = gvkey\n",
    "        \n",
    "    assert len(arr_cs) == len(arr_bs) == len(timelines) == len(a_freq_lst), \"length of 3 outputs dismatch with len(a_freq_lst)\"\n",
    "\n",
    "    with open(data_path+f\"/dataset_top10_freq5_{s_year}_{e_year}.pickle\", \"wb\") as f:\n",
    "        pickle.dump((arr_cs, arr_bs, timelines), f)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.048797Z",
     "start_time": "2021-10-29T16:01:06.044865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'121817'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_freq_idx_to_gvkey_mapping[436]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.078854Z",
     "start_time": "2021-10-29T16:01:06.050450Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path+\"/fv_raw_full_1996_2019.pickle\", 'rb') as f:\n",
    "    fv_full = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.084061Z",
     "start_time": "2021-10-29T16:01:06.080424Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_arr_b_idx(df):\n",
    "    '''\n",
    "    df is the a timeline table of a single firm (the 3rd output of preprocess function)\n",
    "    output is a list\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    sample_df = df.copy()\n",
    "    sub1 = sample_df[(sample_df.EVENT_TYPE == '1')& (sample_df.LOCAL_IDX >=2)] # happened at 3rd time \n",
    "\n",
    "    global_idxs = sub1.GLOBAL_IDX.values # array\n",
    "\n",
    "    arr_b_idxs = []\n",
    "    for global_idx in global_idxs:\n",
    "        sub2 = sample_df[(sample_df.EVENT_TYPE == '3') & (sample_df.GLOBAL_IDX < global_idx)]\n",
    "        arr_b_idx = sub2.iloc[-1, 1]\n",
    "        arr_b_idxs.append(arr_b_idx)\n",
    "\n",
    "    return arr_b_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.090663Z",
     "start_time": "2021-10-29T16:01:06.085281Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_c_t_idx(df):\n",
    "    sample_df = df.copy()\n",
    "    sub1 = sample_df[(sample_df.EVENT_TYPE == '1')& (sample_df.LOCAL_IDX >=2)] \n",
    "    local_idxs = sub1.LOCAL_IDX.values # array\n",
    "    \n",
    "    arr_c_idxs = []\n",
    "    arr_t_idxs = []\n",
    "    for local_idx in local_idxs:\n",
    "        sub2 = sample_df[(sample_df.EVENT_TYPE.isin(['1','2'])) & (sample_df.LOCAL_IDX < local_idx)]\n",
    "        arr_c_idx = sub2.iloc[-1, 1] -1\n",
    "        arr_t_idx = sub2.iloc[-1, 1]\n",
    "        arr_c_idxs.append(arr_c_idx)\n",
    "        arr_t_idxs.append(arr_t_idx)\n",
    "    return arr_c_idxs, arr_t_idxs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.097276Z",
     "start_time": "2021-10-29T16:01:06.091924Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_date(df):\n",
    "    df = df.copy()\n",
    "    def datetime_converter(date_time):\n",
    "        base_time = np.datetime64('1997-01-01')\n",
    "        days_diff = np.datetime64(date_time.date()) - base_time\n",
    "        return days_diff.astype(int)\n",
    "    for idx, row in df.iterrows():\n",
    "        df.loc[idx, 'UPDATE_DATE_int'] = datetime_converter(df.loc[idx, 'UPDATE_DATE'])\n",
    "\n",
    "    #df.sort_values(by = ['UPDATE_DATE']).reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.106317Z",
     "start_time": "2021-10-29T16:01:06.098439Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_negative_time_point(df, base_n_sample=10):\n",
    "    '''\n",
    "    df is timeline + 'UPDATE_DATE_int'\n",
    "    \n",
    "    number of negative samples is corresponding to the number of positive samples (follow the idea of negative smapling in skip-gram)\n",
    "        each word, approx 10 negative samples.\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    max_time = df.UPDATE_DATE_int.values[-1]\n",
    "    sub_df = df[df.EVENT_TYPE.isin(['1', '2']) & (df.LOCAL_IDX >=2)]\n",
    "    min_time = sub_df.UPDATE_DATE_int.values[0]\n",
    "    n_samples = base_n_sample * df.GLOBAL_IDX.values[-1]\n",
    "    samples = np.random.uniform(low=min_time, high=max_time, size=n_samples)\n",
    "    return samples, max_time - min_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.112871Z",
     "start_time": "2021-10-29T16:01:06.107745Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_arr_b_idx_neg(time_samples, df):\n",
    "    '''\n",
    "    df is timeline + 'UPDATE_DATE_int'\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df_b = df[df.EVENT_TYPE == '3']\n",
    "    arr_b_idxs = []\n",
    "    for time in time_samples:\n",
    "        df_b_sub = df_b[df_b.UPDATE_DATE_int<time]\n",
    "        arr_b_idxs.append(df_b_sub.iloc[-1, 1])\n",
    "    \n",
    "    return arr_b_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.119796Z",
     "start_time": "2021-10-29T16:01:06.114357Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_arr_c_t_idx_neg(time_samples, df):\n",
    "    '''\n",
    "    df is timeline + 'UPDATE_DATE_int'\n",
    "    total columns are: [GLOBAL_IDX  LOCAL_IDX UPDATE_DATE EVENT_TYPE  UPDATE_DATE_int]\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df_c = df[df.EVENT_TYPE.isin(['1', '2']) & (df.LOCAL_IDX >=2)]\n",
    "\n",
    "    arr_c_idxs_neg = []\n",
    "    arr_t_neg = []\n",
    "    for time in time_samples:\n",
    "        df_before = df_c[df_c.UPDATE_DATE_int < time]\n",
    "        #print(time)\n",
    "        \n",
    "        arr_c_idx_neg = df_before.iloc[-1, 1] # here do not -1!\n",
    "        previous_time = df_before.iloc[-1, 5]\n",
    "        \n",
    "        arr_c_idxs_neg.append(arr_c_idx_neg)\n",
    "        #print(time, previous_time)\n",
    "        arr_t_neg.append(time - previous_time)\n",
    "    \n",
    "    return arr_c_idxs_neg, np.array(arr_t_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:01:06.129492Z",
     "start_time": "2021-10-29T16:01:06.121359Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_arr_b_c_idx_i(df, s_year, e_year):\n",
    "    '''\n",
    "    df is timeline + 'UPDATE_DATE_int'\n",
    "    total columns are: [GLOBAL_IDX  LOCAL_IDX UPDATE_DATE EVENT_TYPE  UPDATE_DATE_int]\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    # create a year variable\n",
    "    def helper(row):\n",
    "        return row.UPDATE_DATE.year\n",
    "    df['year'] = df.apply(helper, axis=1)\n",
    "    \n",
    "    # qualified self event\n",
    "    sub = df[(df.EVENT_TYPE == '1') & (df.LOCAL_IDX >= 2)]\n",
    "    \n",
    "    yearly = {}\n",
    "    for year in range(s_year, e_year+1):\n",
    "        b_idxs = []\n",
    "        c_idxs = []\n",
    "        sub2 = sub[sub.year == year] # self event at particular year\n",
    "        for _, row in sub2.iterrows():\n",
    "            time = row.UPDATE_DATE_int # float\n",
    "            # back to global df\n",
    "            df_b_before = df[(df.UPDATE_DATE_int < time)&(df.EVENT_TYPE == '3')]\n",
    "            df_c_before = df[(df.UPDATE_DATE_int < time)&(df.EVENT_TYPE.isin(['1','2']))]\n",
    "            idx_b = df_b_before.iloc[-1, 1]\n",
    "            idx_c = df_c_before.iloc[-1, 1] -1\n",
    "            b_idxs.append(idx_b)\n",
    "            c_idxs.append(idx_c)\n",
    "            \n",
    "        yearly[year] = (b_idxs, c_idxs)\n",
    "    \n",
    "    return yearly\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T22:25:15.646189Z",
     "start_time": "2021-10-29T22:25:15.635342Z"
    }
   },
   "outputs": [],
   "source": [
    "def true_tar_idxs_i(timeline, dict_idx):\n",
    "    '''\n",
    "    year loop by self-event year\n",
    "        TNIC related data use year-1\n",
    "    \n",
    "    '''\n",
    "\n",
    "        \n",
    "    # add year to timeline data\n",
    "    df = timeline.copy()\n",
    "    # create a year variable\n",
    "    def helper(row):\n",
    "        return row.UPDATE_DATE.year\n",
    "    df['year'] = df.apply(helper, axis=1)\n",
    "    \n",
    "    # qualified self event\n",
    "    sub = df[(df.EVENT_TYPE == '1') & (df.LOCAL_IDX >= 2)]\n",
    "    \n",
    "    yearly = {}\n",
    "    # loop over self-merge year\n",
    "    for year in range(s_year, e_year+1):\n",
    "        '''\n",
    "        N_i_1 = num of candidate target\n",
    "        N_i_2 = num of self event\n",
    "        '''\n",
    "        N_i_1 = len(gvkey_lsts[year-1]) # all target candidate in TNIC net\n",
    "        b_idxs, c_idxs = dict_idx[year] # the output of ...\n",
    "        N_i_2 = len(b_idxs)\n",
    "        timeline_i = sub[sub.year == year] # only ith year\n",
    "        targets_lst = timeline_i.TGVKEY.values.tolist() # length = N_i_2\n",
    "        assert len(targets_lst) == N_i_2, \"length dismatch with larget lists and N_i_2\"\n",
    "        idx_lst = [key_ind_maps[year-1][tgvkey] for tgvkey in targets_lst]\n",
    "        #one_hot_i = (np.arange(_ == a[...,None]-1).astype(int)\n",
    "        a = np.array(idx_lst)\n",
    "        one_hot_i = (np.arange(N_i_1) == a[...,None]).astype(int)\n",
    "        yearly[year] = one_hot_i\n",
    "    return yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T22:18:46.095532Z",
     "start_time": "2021-10-29T22:18:46.086689Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_node_features(fv_full, gvkey_lsts, key_ind_maps , ind_key_maps, s_year=1997, e_year=2020):\n",
    "    '''\n",
    "    fv_full: raw\n",
    "    gvkey_lsts, key_ind_maps , ind_key_maps: raw\n",
    "    \n",
    "    WARNING: the output yearly's year is self-event's year!!! \n",
    "    '''\n",
    "    # loop self-merge year\n",
    "    yearly = {}\n",
    "    for year in range(s_year, e_year+1): \n",
    "        df_gvkeys = pd.DataFrame({'gvkeys': gvkey_lsts[year-1]})\n",
    "        fv_candidate = fv_full[fv_full.gvkey.isin(gvkey_lsts[year-1]) & (fv_full.year == year-1)]\n",
    "        fv_i = df_gvkeys.merge(fv_candidate, left_on='gvkeys', right_on = 'gvkey', how = \"left\")\n",
    "        fv_i.reset_index(drop=True, inplace=True)\n",
    "        #print(fv_i[:5])\n",
    "        arr = fv_i.iloc[:, 3:].to_numpy()\n",
    "        yearly[year] = arr\n",
    "        assert len(gvkey_lsts[year-1]) == arr.shape[0], \"list and arr shape dismatch\"\n",
    "    \n",
    "    return yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T22:18:46.338291Z",
     "start_time": "2021-10-29T22:18:46.328579Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_net_structure(tmp_data_path, gvkey_lsts, key_ind_maps , ind_key_maps, s_year=1997, e_year=2020):\n",
    "    \n",
    "    yearly = {}    \n",
    "    # loop over self-event year! not TNIC !\n",
    "    for year in range(s_year, e_year+1):     \n",
    "        with open(tmp_data_path+f'/a5_top_10_peers_tnic2_{year-1}.pickle', 'rb') as f:\n",
    "            tnic = pickle.load(f)   \n",
    "            df_all_lst = []\n",
    "            for _,value in tnic.items():\n",
    "                df_all_lst.append(value)\n",
    "            df_all = pd.concat(df_all_lst)\n",
    "            df_net = df_all[['gvkey1', 'gvkey2']]\n",
    "            lst1 = df_net.gvkey1.values.tolist()\n",
    "            lst2 = df_net.gvkey2.values.tolist()\n",
    "            idx1 = [key_ind_maps[year-1][gvkey1] for gvkey1 in lst1]\n",
    "            idx2 = [key_ind_maps[year-1][gvkey2] for gvkey2 in lst2]\n",
    "            arr = np.array([idx1, idx2])\n",
    "            assert arr.shape[0] == 2, \"the dim of output is wrong\"\n",
    "        yearly[year] = arr\n",
    "    return yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create A freq\n",
    "\n",
    "- if change this section, `[arr_cs, arr_bs, timelines]` should be rebuild instead of load from picklle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create everything/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T22:25:19.229911Z",
     "start_time": "2021-10-29T22:25:19.218477Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    ma_dataset = []\n",
    "    for i, gvkey in enumerate(tqdm(a_freq_lst)):\n",
    "        '''\n",
    "        the rest variables are for a specific freq acquirer\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "    \n",
    "        ######### get arr_c and arr_delta_time\n",
    "        arr_c = arr_cs[i]\n",
    "        arr_c = arr_c[1:] # remove the first row!\n",
    "        arr_delta_time = arr_c[:, 0]\n",
    "        # get arr_b\n",
    "        arr_b = arr_bs[i]\n",
    "        \n",
    "        ######## Event data\n",
    "        timeline = convert_date(timelines[i]) # add UPDATE_DATE_int column\n",
    "        #print(timeline.columns)\n",
    "        # arr_b_idx\n",
    "        arr_b_idx = get_arr_b_idx(timeline)\n",
    "        # arr_c_idx and arr_delta_time\n",
    "        arr_c_idx, arr_t_idx = get_c_t_idx(timeline)\n",
    "        arr_delta_time = arr_delta_time[arr_t_idx]\n",
    "        event_data = (arr_c_idx, arr_c_idx, arr_delta_time)\n",
    "        \n",
    "        ######## Non- Event data\n",
    "        # estimate_time_length\n",
    "        samples, estimate_time_length = sample_negative_time_point(timeline)\n",
    "        #  arr_b_idx\n",
    "        non_arr_b_idx =  get_arr_b_idx_neg(samples, timeline)\n",
    "        non_arr_c_idx, non_arr_delta_time = get_arr_c_t_idx_neg(samples, timeline)\n",
    "        non_event_data = (non_arr_b_idx, non_arr_c_idx, non_arr_delta_time)\n",
    "        ######## Choice data dict\n",
    "        dict_idx = get_arr_b_c_idx_i(timeline, s_year, e_year)\n",
    "        true_tar_idxs = true_tar_idxs_i(timeline, dict_idx)\n",
    "        node_feature = get_node_features(fv_full, gvkey_lsts, key_ind_maps , ind_key_maps, s_year, e_year)\n",
    "        net_structure = get_net_structure(tmp_data_path, gvkey_lsts, key_ind_maps , ind_key_maps, s_year, e_year)\n",
    "        choice_data_dict = (dict_idx, true_tar_idxs, node_feature, net_structure)\n",
    "        ma_dataset.append((arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_time_length, choice_data_dict))\n",
    "  \n",
    "    return ma_dataset\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-30T02:21:45.603Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 283/496 [29:43<23:42,  6.68s/it] "
     ]
    }
   ],
   "source": [
    "ma_dataset = create_dataset()\n",
    "with open(data_path+\"/ma_dataset1.pickle\", 'wb') as f:\n",
    "    pickle.dump(ma_dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-30T02:56:26.817Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.685842Z",
     "start_time": "2021-10-29T16:53:17.682664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ma_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.867265Z",
     "start_time": "2021-10-29T16:53:17.687885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 14)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][0].shape  # arr_b (L1, 14)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.889005Z",
     "start_time": "2021-10-29T16:53:17.870179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][1].shape # arr_c (L2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.906177Z",
     "start_time": "2021-10-29T16:53:17.891238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][2].shape # arr_delta_time (L3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.923446Z",
     "start_time": "2021-10-29T16:53:17.908382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ma_dataset[0][3][0]) # arr_b_idx , len = L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.948168Z",
     "start_time": "2021-10-29T16:53:17.925831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ma_dataset[0][3][1])  # arr_c_idx:length = L3\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.965529Z",
     "start_time": "2021-10-29T16:53:17.950359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ma_dataset[0][3][2])  # arr, (L3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:17.982936Z",
     "start_time": "2021-10-29T16:53:17.967925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ma_dataset[0][4][0]) # lst L_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:18.000361Z",
     "start_time": "2021-10-29T16:53:17.985392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ma_dataset[0][4][1]) # lst L_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:18.017597Z",
     "start_time": "2021-10-29T16:53:18.002537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ma_dataset[0][4][2]) # arr L_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:18.034862Z",
     "start_time": "2021-10-29T16:53:18.019784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7804.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][5] # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:18.052794Z",
     "start_time": "2021-10-29T16:53:18.037037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 4, 5, 6, 7, 9, 11, 12, 13])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][6][0][1997]  # dict_idx, arr_b_idx_i: lst, N_i_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:18.070367Z",
     "start_time": "2021-10-29T16:53:18.054950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 7528])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][6][1][1997].size() # torch tensor, one-hot, size = (N_i_2, N_i_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:18.086797Z",
     "start_time": "2021-10-29T16:53:18.072527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7528, 17)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][6][2][1997].shape # node features array: [N_i_1, in_channels_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T16:53:18.104089Z",
     "start_time": "2021-10-29T16:53:18.088889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8252)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_dataset[0][6][3][1997].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d99b08e1309bbda1a7a60a7fe146f82e64d62b20f5527f533414b8454a1fbeeb"
  },
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
