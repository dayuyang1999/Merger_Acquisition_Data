{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:46:46.457528Z",
     "start_time": "2021-09-13T15:46:46.454334Z"
    }
   },
   "source": [
    "https://github.com/dayuyang1999/LDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:46:50.326403Z",
     "start_time": "2021-09-13T15:46:50.323715Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:46:54.432618Z",
     "start_time": "2021-09-13T15:46:54.430014Z"
    }
   },
   "outputs": [],
   "source": [
    "LDG_location = '/home/dalab5/Projects/MA/LDG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:46:57.978661Z",
     "start_time": "2021-09-13T15:46:57.975990Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(LDG_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training log of \"Running the baseline DyRep model [1] on Social Evolution:\"**\n",
    "\n",
    "\n",
    "```bash\n",
    "~~~~~ Script arguments ~~~~~\n",
    "data_dir ./SocialEvolution/\n",
    "dataset social\n",
    "prob 0.8\n",
    "batch_size 200\n",
    "n_hid 32\n",
    "epochs 2\n",
    "seed 1111\n",
    "lr 0.0002\n",
    "lr_decay_step [10]\n",
    "weight 1\n",
    "wdecay 0\n",
    "model dyrep\n",
    "bilinear False\n",
    "bilinear_enc False\n",
    "encoder None\n",
    "sparse False\n",
    "n_rel 2\n",
    "device cuda\n",
    "association CloseFriend\n",
    "resume \n",
    "log_interval 300\n",
    "results results\n",
    "soft_attn False\n",
    "freq False\n",
    "verbose False\n",
    "torch 1.8.0\n",
    "start time: 2021-09-12 16:43:53.505488\n",
    "experiment_ID:  buec-xiaoflambda_505488\n",
    "gitcommit 2036957 \n",
    "\n",
    "loading data from ./SocialEvolution/data_prob0.8.pkl\n",
    "TRAIN\n",
    "Event type=SMS, k=1, number of events=4319\n",
    "Event type=Proximity, k=2, number of events=31011\n",
    "Event type=Calls, k=3, number of events=8187\n",
    "Event type=CloseFriend, k=0, number of events=365\n",
    "TEST\n",
    "Event type=SMS, k=1, number of events=288\n",
    "Event type=Proximity, k=2, number of events=9094\n",
    "Event type=Calls, k=3, number of events=1080\n",
    "Event type=CloseFriend, k=0, number of events=73\n",
    "\n",
    "number of training parameters: 3460\n",
    "\n",
    "Starting training...\n",
    "\n",
    "TRAIN epoch=1/2, batch=220/220, sec/iter: 2.9215, loss=0.287, loss components: [16.929515838623047, 40.473880767822266]\n",
    "the model is saved to results/checkpoints/checkpoint_dygraphs_buec-xiaoflambda_505488_epoch1_batch220.pth.tar\n",
    "\n",
    "TEST batch=53/53, loss=347.308, psi=[0.4648135006427765, 0.45860326290130615], loss1 min/max=0.0195/0.7698, loss2 min/max=0.0036/0.1480, integral time stamps=5000, sec/iter=3.0337\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Epoch 1: results per event type for all test time slots: \n",
    "====== CloseFriend       (73      events):      MAR=36.79+-16.89         HITS_10=0.212+-0.261\n",
    "====== SMS               (288     events):      MAR=19.54+-14.77         HITS_10=0.292+-0.423\n",
    "====== Proximity         (9094    events):      MAR=27.69+-10.18         HITS_10=0.037+-0.164\n",
    "====== Calls             (1080    events):      MAR=28.19+-16.80         HITS_10=0.198+-0.344\n",
    "====== Com               (10462   events):      MAR=27.52+-11.27         HITS_10=0.061+-0.211\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "TRAIN epoch=2/2, batch=220/220, sec/iter: 2.9055, loss=0.275, loss components: [19.30032730102539, 35.67718505859375]\n",
    "the model is saved to results/checkpoints/checkpoint_dygraphs_buec-xiaoflambda_505488_epoch2_batch220.pth.tar\n",
    "\n",
    "TEST batch=53/53, loss=333.265, psi=[0.44680055975914, 0.4191657602787018], loss1 min/max=0.0075/0.6281, loss2 min/max=0.0014/0.1224, integral time stamps=5000, sec/iter=3.1022\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Epoch 2: results per event type for all test time slots: \n",
    "====== CloseFriend       (73      events):      MAR=35.77+-16.61         HITS_10=0.260+-0.288\n",
    "====== SMS               (288     events):      MAR=9.52+-12.56  HITS_10=0.700+-0.434\n",
    "====== Proximity         (9094    events):      MAR=13.24+-11.03         HITS_10=0.443+-0.460\n",
    "====== Calls             (1080    events):      MAR=21.27+-14.90         HITS_10=0.328+-0.403\n",
    "====== Com               (10462   events):      MAR=13.97+-11.81         HITS_10=0.438+-0.457\n",
    "----------------------------------------------------------------------------------------------------\n",
    "end time: 2021-09-12 17:11:25.432984\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Running our latent dynamic graph (LDG) model with a learned graph, sparse prior and biliear interactions:**\n",
    "\n",
    "```bash\n",
    "\n",
    "~~~~~ Script arguments ~~~~~\n",
    "data_dir ./SocialEvolution/\n",
    "dataset social\n",
    "prob 0.8\n",
    "batch_size 200\n",
    "n_hid 32\n",
    "epochs 2\n",
    "seed 1111\n",
    "lr 0.0002\n",
    "lr_decay_step [10]\n",
    "weight 1\n",
    "wdecay 0\n",
    "model dyrep\n",
    "bilinear True\n",
    "bilinear_enc True\n",
    "encoder mlp\n",
    "sparse True\n",
    "n_rel 2\n",
    "device cuda\n",
    "association CloseFriend\n",
    "resume \n",
    "log_interval 300\n",
    "results results\n",
    "soft_attn True\n",
    "freq False\n",
    "verbose False\n",
    "torch 1.8.0\n",
    "start time: 2021-09-12 19:04:08.375378\n",
    "experiment_ID:  buec-xiaoflambda_375378\n",
    "gitcommit 2036957 \n",
    "\n",
    "loading data from ./SocialEvolution/data_prob0.8.pkl\n",
    "TRAIN\n",
    "Event type=SMS, k=1, number of events=4319\n",
    "Event type=Proximity, k=2, number of events=31011\n",
    "Event type=Calls, k=3, number of events=8187\n",
    "Event type=CloseFriend, k=0, number of events=365\n",
    "TEST\n",
    "Event type=SMS, k=1, number of events=288\n",
    "Event type=Proximity, k=2, number of events=9094\n",
    "Event type=Calls, k=3, number of events=1080\n",
    "Event type=CloseFriend, k=0, number of events=73\n",
    "/home/dalab5/Projects/MA/LDG/encoder.py:81: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
    "  nn.init.xavier_normal(m.weight.data)\n",
    "Using factor graph MLP encoder.\n",
    "\n",
    "number of training parameters: 176935\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:49:16.606104Z",
     "start_time": "2021-09-13T15:49:16.602219Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import pandas\n",
    "import itertools\n",
    "import torch\n",
    "import torch.utils\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:48:25.096033Z",
     "start_time": "2021-09-13T15:48:25.089826Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_loader import EventsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:48:37.095676Z",
     "start_time": "2021-09-13T15:48:37.043737Z"
    }
   },
   "outputs": [],
   "source": [
    "class SocialEvolutionDataset(EventsDataset):\n",
    "    '''\n",
    "    Class to load batches for training and testing\n",
    "    '''\n",
    "\n",
    "    FIRST_DATE = datetime.datetime(2008, 9, 11)  # consider events starting from this time\n",
    "    EVENT_TYPES =  ['SMS', 'Proximity', 'Calls']\n",
    "\n",
    "    def __init__(self,\n",
    "                 subj_features,\n",
    "                 data,\n",
    "                 MainAssociation,\n",
    "                 data_train=None,\n",
    "                 verbose=False):\n",
    "        super(SocialEvolutionDataset, self).__init__()\n",
    "\n",
    "        self.subj_features = subj_features\n",
    "        self.data = data\n",
    "        self.verbose = verbose\n",
    "        self.all_events = []\n",
    "        self.event_types_num = {}\n",
    "        self.time_bar = None\n",
    "        self.MainAssociation = MainAssociation\n",
    "        self.TEST_TIMESLOTS = [datetime.datetime(2009, 5, 10), datetime.datetime(2009, 5, 20), datetime.datetime(2009, 5, 31),\n",
    "                               datetime.datetime(2009, 6, 10), datetime.datetime(2009, 6, 20), datetime.datetime(2009, 6, 30)]\n",
    "        self.FIRST_DATE = SocialEvolutionDataset.FIRST_DATE\n",
    "        self.event_types = SocialEvolutionDataset.EVENT_TYPES\n",
    "\n",
    "        k = 1  # k >= 1 for communication events\n",
    "        print(data.split.upper())\n",
    "        for t in self.event_types:\n",
    "            print('Event type={}, k={}, number of events={}'.format(t, k, len(data.EVENT_TYPES[t].tuples)))\n",
    "\n",
    "            events = list(filter(lambda x: x[3].toordinal() >= self.FIRST_DATE.toordinal(),\n",
    "                                 data.EVENT_TYPES[t].tuples))\n",
    "            self.all_events.extend(events)\n",
    "            self.event_types_num[t] = k\n",
    "            k += 1\n",
    "\n",
    "        n = len(self.all_events)\n",
    "        self.N_nodes = subj_features.shape[0]\n",
    "\n",
    "        if data.split == 'train':\n",
    "            Adj_all, keys, Adj_all_last = self.get_Adjacency()\n",
    "\n",
    "            if self.verbose:\n",
    "                print('initial and final associations', self.MainAssociation, Adj_all.sum(), Adj_all_last.sum(),\n",
    "                      np.allclose(Adj_all, Adj_all_last))\n",
    "\n",
    "\n",
    "        # Initial topology\n",
    "        if len(list(data.Adj.keys())) > 0:\n",
    "\n",
    "            keys = sorted(list(data.Adj[list(data.Adj.keys())[0]].keys()))  # relation keys\n",
    "            keys.remove(MainAssociation)\n",
    "            keys = [MainAssociation] + keys  # to make sure CloseFriend goes first\n",
    "\n",
    "            k = 0  # k <= 0 for association events\n",
    "            for rel in keys:\n",
    "\n",
    "                if rel != MainAssociation:\n",
    "                    continue\n",
    "                if data_train is None:\n",
    "                    date = sorted(list(data.Adj.keys()))[0]  # first date\n",
    "                    Adj_prev = data.Adj[date][rel]\n",
    "                else:\n",
    "                    date = sorted(list(data_train.Adj.keys()))[-1]  # last date of the training set\n",
    "                    Adj_prev = data_train.Adj[date][rel]\n",
    "                self.event_types_num[rel] = k\n",
    "\n",
    "                N = Adj_prev.shape[0]\n",
    "\n",
    "                # Associative events\n",
    "                for date_id, date in enumerate(sorted(list(data.Adj.keys()))):  # start from the second survey\n",
    "                    if date.toordinal() >= self.FIRST_DATE.toordinal():\n",
    "                        # for rel_id, rel in enumerate(sorted(list(dygraphs.Adj[date].keys()))):\n",
    "                        assert data.Adj[date][rel].shape[0] == N\n",
    "                        for u in range(N):\n",
    "                            for v in range(u + 1, N):\n",
    "                                # if two nodes become friends, add the event\n",
    "                                if data.Adj[date][rel][u, v] > 0 and Adj_prev[u, v] == 0:\n",
    "                                    assert u != v, (u, v, k)\n",
    "                                    self.all_events.append((u, v, rel, date))\n",
    "\n",
    "                    Adj_prev = data.Adj[date][rel]\n",
    "\n",
    "                # print(data.split, rel, len(self.all_events) - n)\n",
    "                print('Event type={}, k={}, number of events={}'.format(rel, k, len(self.all_events) - n))\n",
    "                n = len(self.all_events)\n",
    "                k -= 1\n",
    "\n",
    "        self.all_events = sorted(self.all_events, key=lambda x: int(x[3].timestamp()))\n",
    "\n",
    "        if self.verbose:\n",
    "            print('%d events' % len(self.all_events))\n",
    "            print('last 10 events:')\n",
    "            for event in self.all_events[-10:]:\n",
    "                print(event)\n",
    "\n",
    "        self.n_events = len(self.all_events)\n",
    "\n",
    "        H_train = np.zeros((N, N))\n",
    "        c = 0\n",
    "        for e in self.all_events:\n",
    "            H_train[e[0], e[1]] += 1\n",
    "            H_train[e[1], e[0]] += 1\n",
    "            c += 1\n",
    "        if self.verbose:\n",
    "            print('H_train', c, H_train.max(), H_train.min(), H_train.std())\n",
    "        self.H_train = H_train\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(data_dir, prob, dump=True):\n",
    "        data_file = pjoin(data_dir, 'data_prob%s.pkl' % prob)\n",
    "        if os.path.isfile(data_file):\n",
    "            print('loading data from %s' % data_file)\n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        else:\n",
    "            data = {'initial_embeddings': SubjectsReader(pjoin(data_dir, 'Subjects.csv')).features_onehot}\n",
    "            for split in ['train', 'test']:\n",
    "                data.update(\n",
    "                    {split: SocialEvolution(data_dir, split=split, MIN_EVENT_PROB=prob)})\n",
    "            if dump:\n",
    "                # dump data files to avoid their generation again\n",
    "                print('saving data to %s' % data_file)\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(data, f, protocol=2)  # for compatibility\n",
    "        return data\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        dates = sorted(list(self.data.Adj.keys()))\n",
    "        Adj_all = self.data.Adj[dates[0]]\n",
    "        Adj_all_last = self.data.Adj[dates[-1]]\n",
    "        # Adj_friends = Adj_all[self.MainAssociation].copy()\n",
    "        if multirelations:\n",
    "            keys = sorted(list(Adj_all.keys()))\n",
    "            keys.remove(self.MainAssociation)\n",
    "            keys = [self.MainAssociation] + keys  # to make sure CloseFriend goes first\n",
    "            Adj_all = np.stack([Adj_all[rel].copy() for rel in keys], axis=2)\n",
    "            Adj_all_last = np.stack([Adj_all_last[rel].copy() for rel in keys], axis=2)\n",
    "        else:\n",
    "            keys = [self.MainAssociation]\n",
    "            Adj_all = Adj_all[self.MainAssociation].copy()\n",
    "            Adj_all_last = Adj_all_last[self.MainAssociation].copy()\n",
    "\n",
    "        return Adj_all, keys, Adj_all_last\n",
    "\n",
    "\n",
    "    def time_to_onehot(self, d):\n",
    "        x = []\n",
    "        for t, max_t in [(d.weekday(), 7), (d.hour, 24), (d.minute, 60), (d.second, 60)]:\n",
    "            x_t = np.zeros(max_t)\n",
    "            x_t[t] = 1\n",
    "            x.append(x_t)\n",
    "        return np.concatenate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:48:39.623056Z",
     "start_time": "2021-09-13T15:48:39.620234Z"
    }
   },
   "outputs": [],
   "source": [
    "from social_data_loader import SubjectsReader, SocialEvolution, CSVReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:49:33.133898Z",
     "start_time": "2021-09-13T15:49:19.612003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ./SocialEvolution/data_prob0.8.pkl\n",
      "TRAIN\n",
      "Event type=SMS, k=1, number of events=4319\n",
      "Event type=Proximity, k=2, number of events=31011\n",
      "Event type=Calls, k=3, number of events=8187\n",
      "Event type=CloseFriend, k=0, number of events=365\n",
      "TEST\n",
      "Event type=SMS, k=1, number of events=288\n",
      "Event type=Proximity, k=2, number of events=9094\n",
      "Event type=Calls, k=3, number of events=1080\n",
      "Event type=CloseFriend, k=0, number of events=73\n"
     ]
    }
   ],
   "source": [
    "data_dir = './SocialEvolution'\n",
    "data = SocialEvolutionDataset.load_data(data_dir, 0.8)\n",
    "train_set = SocialEvolutionDataset(data['initial_embeddings'], data['train'], 'CloseFriend', verbose=False)\n",
    "test_set = SocialEvolutionDataset(data['initial_embeddings'], data['test'], 'CloseFriend',\n",
    "                            data_train=data['train'], verbose=False)\n",
    "initial_embeddings = data['initial_embeddings'].copy()\n",
    "A_initial = train_set.get_Adjacency()[0]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original Data Desceiption in the paper\n",
    "\n",
    "Size:\n",
    "- 83 nodes\n",
    "- millions of events $o^{t}=(u, v, \\tau, k)$\n",
    " \n",
    "\n",
    "\n",
    "Interaction:\n",
    "- commuication: [SMS, Proximity, Call]\n",
    "- association: CLoseFriend\n",
    "\n",
    "\n",
    "Training\n",
    "- Sep 2008 ~ April 2009, 43000 communication events\n",
    "\n",
    "Test\n",
    "- May 2009 ~ June 2009, 10000  communication events\n",
    "\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/AVRtAbYF-MQlnuYgnVaVIHh6PqkFjCiDgI5j8iaTNOc.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Randomly Explore Data Looking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:49:33.148822Z",
     "start_time": "2021-09-13T15:49:33.137294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'] == train_set.data # data is an attributes of train_set object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:49:54.332451Z",
     "start_time": "2021-09-13T15:49:54.327026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 0, 'SMS', datetime.datetime(2008, 1, 1, 15, 0, 25)),\n",
       " (60, 0, 'SMS', datetime.datetime(2008, 1, 6, 12, 1, 36)),\n",
       " (60, 0, 'SMS', datetime.datetime(2008, 3, 31, 19, 49, 18)),\n",
       " (0, 60, 'SMS', datetime.datetime(2008, 4, 26, 18, 35, 34)),\n",
       " (0, 60, 'SMS', datetime.datetime(2008, 4, 26, 19, 35, 34)),\n",
       " (0, 60, 'SMS', datetime.datetime(2008, 4, 26, 20, 35, 34)),\n",
       " (0, 60, 'SMS', datetime.datetime(2008, 6, 20, 9, 30, 44)),\n",
       " (0, 60, 'SMS', datetime.datetime(2008, 6, 20, 10, 30, 44)),\n",
       " (0, 60, 'SMS', datetime.datetime(2008, 6, 20, 11, 30, 44)),\n",
       " (0, 60, 'SMS', datetime.datetime(2008, 7, 8, 3, 52, 20))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].EVENT_TYPES['SMS'].tuples[:10] # a list contains events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:49:59.233224Z",
     "start_time": "2021-09-13T15:49:59.229348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4319\n",
      "31011\n",
      "8187\n"
     ]
    }
   ],
   "source": [
    "for t in train_set.event_types:\n",
    "    print(len(data['train'].EVENT_TYPES[t].tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:50:06.231143Z",
     "start_time": "2021-09-13T15:50:06.224669Z"
    }
   },
   "outputs": [],
   "source": [
    "Adj_all_start, keys, Adj_all_last = train_set.get_Adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:50:13.081965Z",
     "start_time": "2021-09-13T15:50:13.077004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adj_all_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:50:16.864379Z",
     "start_time": "2021-09-13T15:50:16.854771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adj_all_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:50:21.196879Z",
     "start_time": "2021-09-13T15:50:21.188370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CloseFriend']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys # what cause a new adjecent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:50:26.986587Z",
     "start_time": "2021-09-13T15:50:26.976801Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c2c19b72ebf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# call __getitem__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/MA/LDG/data_loader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# most recent previous time for all nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtime_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "train_set[0] # call __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
