{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dayuyang1999/LDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T23:02:18.952697Z",
     "start_time": "2021-09-12T23:02:18.949935Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move to where LDG lcoated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T23:01:35.153477Z",
     "start_time": "2021-09-12T23:01:35.150932Z"
    }
   },
   "outputs": [],
   "source": [
    "LDG_location = '/home/dalab5/Projects/MA/LDG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T23:01:35.358628Z",
     "start_time": "2021-09-12T23:01:35.355924Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(LDG_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- [ ]  train on LDG\n",
    "    - [ ]  use original data, successfully trained result\n",
    "        - [ ]  Data Part\n",
    "            - [ ]  Original Data Structure\n",
    "            - [ ]  how to load data to the model\n",
    "        - [ ]  Model Part\n",
    "        - [ ]  Training Part\n",
    "    - [ ]  use MA events data(just interaction, no financial variables),  successfully trained result\n",
    "    - [ ]  use both fin_var and interactions, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Github with Dyrep**\n",
    "\n",
    "```bash\n",
    "~~~~~ Script arguments ~~~~~\n",
    "data_dir ./Github\n",
    "dataset github\n",
    "prob 0.8\n",
    "batch_size 200\n",
    "n_hid 32\n",
    "epochs 2\n",
    "seed 1111\n",
    "lr 0.0002\n",
    "lr_decay_step [10]\n",
    "weight 1\n",
    "wdecay 0\n",
    "model dyrep\n",
    "bilinear False\n",
    "bilinear_enc False\n",
    "encoder None\n",
    "sparse False\n",
    "n_rel 2\n",
    "device cuda\n",
    "association CloseFriend\n",
    "resume \n",
    "log_interval 300\n",
    "results results\n",
    "soft_attn False\n",
    "freq False\n",
    "verbose False\n",
    "torch 1.8.0\n",
    "start time: 2021-09-12 19:55:46.631513\n",
    "experiment_ID:  buec-xiaoflambda_631513\n",
    "gitcommit 2036957 \n",
    "\n",
    "{'PushEvent': 0, 'WatchEvent': 1, 'ForkEvent': 2, 'IssuesEvent': 3, 'FollowEvent': 4, 'PullRequestEvent': 5, 'IssueCommentEvent': 6, 'CommitCommentEvent': 7}\n",
    "{0: 'PushEvent', 1: 'WatchEvent', 2: 'ForkEvent', 3: 'IssuesEvent', 4: 'FollowEvent', 5: 'PullRequestEvent', 6: 'IssueCommentEvent', 7: 'CommitCommentEvent'}\n",
    "\n",
    "A_initial 298.0\n",
    "A_last 1420.0 \n",
    "\n",
    "\n",
    "TRAIN\n",
    "11644 events between 284 users loaded\n",
    "0 communication events\n",
    "0 assocition events\n",
    "{'PushEvent': 0, 'WatchEvent': 1, 'ForkEvent': 2, 'IssuesEvent': 3, 'FollowEvent': 4, 'PullRequestEvent': 5, 'IssueCommentEvent': 6, 'CommitCommentEvent': 7}\n",
    "{0: 'PushEvent', 1: 'WatchEvent', 2: 'ForkEvent', 3: 'IssuesEvent', 4: 'FollowEvent', 5: 'PullRequestEvent', 6: 'IssueCommentEvent', 7: 'CommitCommentEvent'}\n",
    "\n",
    "A_initial 298.0\n",
    "A_last 1420.0 \n",
    "\n",
    "\n",
    "TEST\n",
    "9082 events between 284 users loaded\n",
    "0 communication events\n",
    "0 assocition events\n",
    "warning: Github has only one relation type (FollowEvent), so multirelations are ignored\n",
    "\n",
    "number of training parameters: 3460\n",
    "\n",
    "Starting training...\n",
    "\n",
    "TRAIN epoch=1/2, batch=59/59, sec/iter: 2.7769, loss=0.459, loss components: [28.97666358947754, 62.74485778808594]\n",
    "the model is saved to results/checkpoints/checkpoint_dygraphs_buec-xiaoflambda_631513_epoch1_batch59.pth.tar\n",
    "\n",
    "TEST batch=46/46, loss=420.642, psi=[0.48852670192718506, 0.4910774827003479], loss1 min/max=0.0485/0.8133, loss2 min/max=0.0017/0.1764, integral time stamps=5000, sec/iter=4.5002\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Epoch 1: results per event type for all test time slots: \n",
    "====== FollowEvent       (415     events):      MAR=101.40+-55.63        HITS_10=0.160+-0.246\n",
    "====== ForkEvent         (463     events):      MAR=119.34+-66.05        HITS_10=0.143+-0.228\n",
    "====== PushEvent         (392     events):      MAR=178.92+-74.12        HITS_10=0.062+-0.173\n",
    "====== WatchEvent        (4365    events):      MAR=97.42+-58.44         HITS_10=0.230+-0.261\n",
    "====== IssuesEvent       (666     events):      MAR=167.27+-77.83        HITS_10=0.082+-0.187\n",
    "====== IssueCommentEvent         (2084    events):      MAR=154.70+-77.26        HITS_10=0.114+-0.216\n",
    "====== PullRequestEvent  (567     events):      MAR=159.00+-79.06        HITS_10=0.071+-0.182\n",
    "====== CommitCommentEvent        (130     events):      MAR=145.67+-75.74        HITS_10=0.092+-0.213\n",
    "====== Com               (8667    events):      MAR=126.17+-74.46        HITS_10=0.166+-0.244\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "TRAIN epoch=2/2, batch=59/59, sec/iter: 2.8696, loss=0.426, loss components: [28.032928466796875, 57.12016296386719]\n",
    "the model is saved to results/checkpoints/checkpoint_dygraphs_buec-xiaoflambda_631513_epoch2_batch59.pth.tar\n",
    "\n",
    "TEST batch=46/46, loss=396.081, psi=[0.48039165139198303, 0.4789699912071228], loss1 min/max=0.0346/0.7851, loss2 min/max=0.0013/0.1696, integral time stamps=5000, sec/iter=4.7082\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Epoch 2: results per event type for all test time slots: \n",
    "====== FollowEvent       (415     events):      MAR=100.13+-55.51        HITS_10=0.159+-0.243\n",
    "====== ForkEvent         (463     events):      MAR=116.85+-64.42        HITS_10=0.145+-0.229\n",
    "====== PushEvent         (392     events):      MAR=176.30+-72.72        HITS_10=0.064+-0.171\n",
    "====== WatchEvent        (4365    events):      MAR=95.65+-57.25         HITS_10=0.233+-0.262\n",
    "====== IssuesEvent       (666     events):      MAR=164.37+-76.64        HITS_10=0.087+-0.192\n",
    "====== IssueCommentEvent         (2084    events):      MAR=151.87+-76.19        HITS_10=0.117+-0.218\n",
    "====== PullRequestEvent  (567     events):      MAR=156.81+-77.34        HITS_10=0.074+-0.183\n",
    "====== CommitCommentEvent        (130     events):      MAR=142.25+-74.63        HITS_10=0.092+-0.213\n",
    "====== Com               (8667    events):      MAR=123.93+-73.16        HITS_10=0.169+-0.245\n",
    "----------------------------------------------------------------------------------------------------\n",
    "end time: 2021-09-12 20:08:47.127488\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T23:15:44.667072Z",
     "start_time": "2021-09-12T23:15:44.062491Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Data \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data Structure\n",
    "\n",
    "#### Github\n",
    "\n",
    "First, explore Github Dataset.\n",
    "\n",
    "The Dataset Description from the paper:\n",
    "- very large network\n",
    "- sparse interaction events\n",
    "    - LDG learn embedding from interactions; So It must perform bad in this dataset\n",
    "\n",
    "\n",
    "Modification of original dataset from the researcher:\n",
    "- extract a dense subnet\n",
    "- each user has >200 communication and >7 association\n",
    "\n",
    "Detail:\n",
    "- Init: From 2011 to 2012, \"Follow\" event as init association\n",
    "- Training: Dec 2012 to Aug 2013, 284 nodes and 10000 events\n",
    "    - both `communication` and `association` events are used for training\n",
    "        - `communication` is defined as including [Watch, Star, Fork, Push, Issues, IssueComment, PullRequest, Commit.]\n",
    "- Test: Sep 2013 to Dec 2013, 8000 events\n",
    "    - give tuple $(u, v, \\tau, k)$ (the positive sample) compute the conditional density of $u$ with all other nodes and rank them (MAR). See how top $v$ ranks.\n",
    "        - $u$: node; $\\tau$: time point of event; $k$: event type\n",
    "       \n",
    "        \n",
    "\n",
    "Evaluation:\n",
    "- only on `communication` events, \n",
    "    - cannot learn `association` events well since its sparsity.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T23:46:52.028711Z",
     "start_time": "2021-09-12T23:46:51.920861Z"
    }
   },
   "outputs": [],
   "source": [
    "git_2011_2012 = pd.read_pickle('./Github/github_284users_follow_2011_2012.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T23:47:30.248880Z",
     "start_time": "2021-09-12T23:47:30.233039Z"
    }
   },
   "outputs": [],
   "source": [
    "git_2013 = pd.read_pickle('./Github/github_284users_events_2013.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About Training data `git_2011_2012`\n",
    "\n",
    "A dict: \n",
    "    - keys: 259 username\n",
    "    - item: a list of events\n",
    "        - element: a dict contain 3 info: time, typeofevent, login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:06:12.604926Z",
     "start_time": "2021-09-13T00:06:12.601526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(git_2011_2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:06:14.360815Z",
     "start_time": "2021-09-13T00:06:14.356858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(git_2011_2012) # the key of dict is the username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:14:24.573944Z",
     "start_time": "2021-09-13T00:14:24.567887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgs', 'gregstallings', 'L42y']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(git_2011_2012.keys())[:3] # choose first 3 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:14:43.707097Z",
     "start_time": "2021-09-13T00:14:43.702531Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'created_at': '2012/03/11 00:54:49 -0800',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'stephenmcd'},\n",
       " {'created_at': '2012/03/20 07:16:28 -0700',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'derailed'},\n",
       " {'created_at': '2012/04/22 05:42:30 -0700',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'Nikku'},\n",
       " {'created_at': '2012/06/01 01:08:12 -0700',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'plusjade'},\n",
       " {'created_at': '2012-06-25T06:23:08-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'dotpy'},\n",
       " {'created_at': '2012-07-01T08:14:58-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'mbostock'},\n",
       " {'created_at': '2012-07-21T00:19:31-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'dblock'},\n",
       " {'created_at': '2012-07-21T04:59:43-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'mperham'},\n",
       " {'created_at': '2012-07-30T00:40:47-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'mongoid'},\n",
       " {'created_at': '2012-08-01T14:40:06-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'mattions'},\n",
       " {'created_at': '2012-08-12T03:03:18-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'potomak'},\n",
       " {'created_at': '2012-08-14T13:20:52-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'alexeypetrushin'},\n",
       " {'created_at': '2012-10-12T05:28:13-07:00',\n",
       "  'type': 'FollowEvent',\n",
       "  'login': 'andreareginato'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_2011_2012['lgs'] # the __item__ is a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:17:46.422275Z",
     "start_time": "2021-09-13T00:17:46.415485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "print(len(git_2011_2012['lgs']))\n",
    "print(len(git_2011_2012['gregstallings'])) \n",
    "# of course, each person has different number of events history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:15:18.450965Z",
     "start_time": "2021-09-13T00:15:18.446717Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2012/03/11 00:54:49 -0800',\n",
       " 'type': 'FollowEvent',\n",
       " 'login': 'stephenmcd'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_2011_2012['lgs'][0] # each element of a list is a dict, contains 3 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About test data git_2013\n",
    "\n",
    "\n",
    "A tuple, the first element is like `git_2011_2012`, a dict that contains all interactions classified by username.\n",
    "\n",
    "- Each user has its own list of events.\n",
    "    - an event has 3 keys: time, eventtype, login\n",
    "\n",
    "The 2nd element is just a mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:01:44.695458Z",
     "start_time": "2021-09-13T01:01:44.692998Z"
    }
   },
   "outputs": [],
   "source": [
    "users_events, event_types = git_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:03:50.947390Z",
     "start_time": "2021-09-13T01:03:50.943634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(users_events.keys()))\n",
    "print(len(event_types.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:07:17.546377Z",
     "start_time": "2021-09-13T01:07:17.542067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PushEvent': 0,\n",
       " 'WatchEvent': 1,\n",
       " 'ForkEvent': 2,\n",
       " 'IssuesEvent': 3,\n",
       " 'FollowEvent': 4,\n",
       " 'PullRequestEvent': 5,\n",
       " 'IssueCommentEvent': 6,\n",
       " 'CommitCommentEvent': 7}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_types # a mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:02:29.248781Z",
     "start_time": "2021-09-13T01:02:29.244578Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TheHydroImpulse', 'jkroso', 'juliangruber']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(users_events.keys())[:3] # 3 sample users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:03:06.851980Z",
     "start_time": "2021-09-13T01:03:06.844985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "print(len(users_events['juliangruber']))\n",
    "print(len(users_events['jkroso'])) # differnt user has different length of events history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:03:14.460389Z",
     "start_time": "2021-09-13T01:03:14.456253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 1357289009.0, 'type': 1, 'login': 'hij1nx'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_events['juliangruber'][0] # the first event of user juliangruber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Load data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:50:43.879742Z",
     "start_time": "2021-09-13T15:50:43.872113Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from datetime import datetime, timezone\n",
    "import dateutil.parser\n",
    "from data_loader import EventsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:42.501605Z",
     "start_time": "2021-09-13T20:29:42.493160Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class EventsDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Base class for event datasets\n",
    "    '''\n",
    "    def __init__(self, TZ=None):\n",
    "        self.TZ = TZ  # timezone.utc\n",
    "\n",
    "        # Implement here these fields (see examples in actual datasets):\n",
    "        # self.FIRST_DATE = datetime()\n",
    "        # self.TEST_TIMESLOTS = []\n",
    "        # self.N_nodes = 100\n",
    "        # self.A_initial = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        # self.A_last = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        #\n",
    "        # self.all_events = []\n",
    "        # self.n_events = len(self.all_events)\n",
    "        #\n",
    "        # self.event_types = ['communication event']\n",
    "        # self.event_types_num = {'association event': 0}\n",
    "        # k = 1  # k >= 1 for communication events\n",
    "        # for t in self.event_types:\n",
    "        #     self.event_types_num[t] = k\n",
    "        #     k += 1\n",
    "        \n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        return None, None, Nonet\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_events\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tpl = self.all_events[index]\n",
    "        u, v, rel, time_cur = tpl  # current time of event index \n",
    "\n",
    "        # Compute time delta in seconds (t_p - \\bar{t}_p_j) that will be fed to W_t\n",
    "        time_delta_uv = np.zeros((2, 4))  # two nodes x 4 values\n",
    "\n",
    "        # most recent previous time for all nodes\n",
    "        #print('self.time_bar', self.time_bar)\n",
    "        time_bar = self.time_bar.copy()\n",
    "        \n",
    "    \n",
    "        \n",
    "        assert u != v, (tpl, rel)\n",
    "\n",
    "        for c, j in enumerate([u, v]):\n",
    "            t = datetime.fromtimestamp(self.time_bar[j], tz=self.TZ)\n",
    "            print(\" t.toordinal()\",  t.toordinal(), \"\\n\", \"self.FIRST_DATE.toordinal()\", self.FIRST_DATE.toordinal(), '\\n')\n",
    "            if t.toordinal() >= self.FIRST_DATE.toordinal():  # assume no events before FIRST_DATE\n",
    "                \n",
    "                td = time_cur - t\n",
    "                time_delta_uv[c] = np.array([td.days,  # total number of days, still can be a big number\n",
    "                                             td.seconds // 3600,  # hours, max 24\n",
    "                                             (td.seconds // 60) % 60,  # minutes, max 60\n",
    "                                             td.seconds % 60],  # seconds, max 60\n",
    "                                            np.float)\n",
    "                # assert time_delta_uv.min() >= 0, (index, tpl, time_delta_uv[c], node_global_time[j])\n",
    "            else:\n",
    "                raise ValueError('unexpected result', t, self.FIRST_DATE)\n",
    "            self.time_bar[j] = time_cur.timestamp()  # last time stamp for nodes u and v\n",
    "\n",
    "        k = self.event_types_num[rel]\n",
    "\n",
    "        # sanity checks\n",
    "        assert np.float64(time_cur.timestamp()) == time_cur.timestamp(), (\n",
    "        np.float64(time_cur.timestamp()), time_cur.timestamp())\n",
    "        time_cur = np.float64(time_cur.timestamp())\n",
    "        time_bar = time_bar.astype(np.float64)\n",
    "        time_cur = torch.from_numpy(np.array([time_cur])).double()\n",
    "        assert time_bar.max() <= time_cur, (time_bar.max(), time_cur)\n",
    "        return u, v, time_delta_uv, k, time_bar, time_cur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:46.646600Z",
     "start_time": "2021-09-13T20:29:46.620836Z"
    }
   },
   "outputs": [],
   "source": [
    "def iso_parse(dt):\n",
    "    # return datetime.fromisoformat(dt)  # python >= 3.7\n",
    "    return dateutil.parser.isoparse(dt)\n",
    "\n",
    "class GithubDataset(EventsDataset):\n",
    "\n",
    "    def __init__(self, split, data_dir='./Github'):\n",
    "        super(GithubDataset, self).__init__()\n",
    "\n",
    "        if split == 'train':\n",
    "            time_start = 0\n",
    "            time_end = datetime(2013, 8, 31, tzinfo=self.TZ).toordinal()\n",
    "        elif split == 'test':\n",
    "            time_start = datetime(2013, 9, 1, tzinfo=self.TZ).toordinal()\n",
    "            time_end = datetime(2014, 1, 1, tzinfo=self.TZ).toordinal()\n",
    "        else:\n",
    "            raise ValueError('invalid split', split)\n",
    "\n",
    "        self.FIRST_DATE = datetime(2012, 12, 28, tzinfo=self.TZ)\n",
    "\n",
    "        self.TEST_TIMESLOTS = [datetime(2013, 9, 1, tzinfo=self.TZ),\n",
    "                               datetime(2013, 9, 25, tzinfo=self.TZ),\n",
    "                               datetime(2013, 10, 20, tzinfo=self.TZ),\n",
    "                               datetime(2013, 11, 15, tzinfo=self.TZ),\n",
    "                               datetime(2013, 12, 10, tzinfo=self.TZ),\n",
    "                               datetime(2014, 1, 1, tzinfo=self.TZ)]\n",
    "\n",
    "\n",
    "\n",
    "        with open(os.path.join(data_dir, 'github_284users_events_2013.pkl'), 'rb') as f:\n",
    "            users_events, event_types = pickle.load(f)\n",
    "\n",
    "        with open(os.path.join(data_dir, 'github_284users_follow_2011_2012.pkl'), 'rb') as f:\n",
    "            users_follow = pickle.load(f)\n",
    "\n",
    "        print(event_types)\n",
    "\n",
    "        self.events2name = {}\n",
    "        for e in event_types:\n",
    "            self.events2name[event_types[e]] = e\n",
    "        print(self.events2name)\n",
    "\n",
    "        self.event_types = ['ForkEvent', 'PushEvent', 'WatchEvent', 'IssuesEvent', 'IssueCommentEvent',\n",
    "                           'PullRequestEvent', 'CommitCommentEvent']\n",
    "        \n",
    "        self.assoc_types = ['FollowEvent']\n",
    "        self.is_comm = lambda d: self.events2name[d['type']] in self.event_types\n",
    "        self.is_assoc = lambda d: self.events2name[d['type']] in self.assoc_types\n",
    "\n",
    "        user_ids = {}\n",
    "        for id, user in enumerate(sorted(users_events)):\n",
    "            user_ids[user] = id\n",
    "        \n",
    "        self.N_nodes = len(user_ids)\n",
    "\n",
    "        self.A_initial = np.zeros((self.N_nodes, self.N_nodes))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for user in users_follow: \n",
    "            for e in users_follow[user]:\n",
    "                assert e['type'] in self.assoc_types, e['type']\n",
    "                if e['login'] in users_events:\n",
    "                    self.A_initial[user_ids[user], user_ids[e['login']]] = 1\n",
    "\n",
    "        self.A_last = np.zeros((self.N_nodes, self.N_nodes))\n",
    "        for user in users_events:\n",
    "            for e in users_events[user]:\n",
    "                if self.events2name[e['type']] in self.assoc_types:\n",
    "                    self.A_last[user_ids[user], user_ids[e['login']]] = 1\n",
    "\n",
    "        print('\\nA_initial', np.sum(self.A_initial))\n",
    "        print('A_last', np.sum(self.A_last), '\\n')\n",
    "\n",
    "        all_events = []\n",
    "        for user in users_events:\n",
    "            if user not in user_ids:\n",
    "                continue\n",
    "            user_id = user_ids[user]\n",
    "            for ind, event in enumerate(users_events[user]):\n",
    "                event['created_at'] = datetime.fromtimestamp(event['created_at'])\n",
    "                if event['created_at'].toordinal() >= time_start and event['created_at'].toordinal() <= time_end:\n",
    "                    if 'owner' in event:\n",
    "                        if event['owner'] not in user_ids:\n",
    "                            continue\n",
    "                        user_id2 = user_ids[event['owner']]\n",
    "                    elif 'login' in event:\n",
    "                        if event['login'] not in user_ids:\n",
    "                            continue\n",
    "                        user_id2 = user_ids[event['login']]\n",
    "                    else:\n",
    "                        raise ValueError('invalid event', event)\n",
    "                    if user_id != user_id2:\n",
    "                        all_events.append((user_id, user_id2,\n",
    "                                           self.events2name[event['type']], event['created_at']))\n",
    "        self.user_ids = user_ids\n",
    "        self.all_events = sorted(all_events, key=lambda t: t[3].timestamp())\n",
    "        print('\\n%s' % split.upper())\n",
    "        print('%d events between %d users loaded' % (len(self.all_events), self.N_nodes))\n",
    "        print('%d communication events' % (len([t for t in self.all_events if t[2] == 1])))\n",
    "        print('%d assocition events' % (len([t for t in self.all_events if t[2] == 0])))\n",
    "\n",
    "        self.event_types_num = {self.assoc_types[0]: 0}\n",
    "        k = 1  # k >= 1 for communication events\n",
    "        for t in self.event_types:\n",
    "            self.event_types_num[t] = k\n",
    "            k += 1\n",
    "\n",
    "        self.n_events = len(self.all_events)\n",
    "        #self.time_bar = len(user_ids)*[datetime(2012, 12, 29, tzinfo=timezone.utc).timestamp() ]\n",
    "        time_bar = np.zeros(len(user_ids))\n",
    "        time_bar.fill(datetime(2012, 12, 29, tzinfo=timezone.utc).timestamp())\n",
    "        self.time_bar = time_bar\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        if multirelations:\n",
    "            print('warning: Github has only one relation type (FollowEvent), so multirelations are ignored')\n",
    "        return self.A_initial, self.assoc_types, self.A_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:46.922225Z",
     "start_time": "2021-09-13T20:29:46.919940Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './Github'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:47.480919Z",
     "start_time": "2021-09-13T20:29:47.165663Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PushEvent': 0, 'WatchEvent': 1, 'ForkEvent': 2, 'IssuesEvent': 3, 'FollowEvent': 4, 'PullRequestEvent': 5, 'IssueCommentEvent': 6, 'CommitCommentEvent': 7}\n",
      "{0: 'PushEvent', 1: 'WatchEvent', 2: 'ForkEvent', 3: 'IssuesEvent', 4: 'FollowEvent', 5: 'PullRequestEvent', 6: 'IssueCommentEvent', 7: 'CommitCommentEvent'}\n",
      "\n",
      "A_initial 298.0\n",
      "A_last 1420.0 \n",
      "\n",
      "\n",
      "TRAIN\n",
      "11644 events between 284 users loaded\n",
      "0 communication events\n",
      "0 assocition events\n",
      "{'PushEvent': 0, 'WatchEvent': 1, 'ForkEvent': 2, 'IssuesEvent': 3, 'FollowEvent': 4, 'PullRequestEvent': 5, 'IssueCommentEvent': 6, 'CommitCommentEvent': 7}\n",
      "{0: 'PushEvent', 1: 'WatchEvent', 2: 'ForkEvent', 3: 'IssuesEvent', 4: 'FollowEvent', 5: 'PullRequestEvent', 6: 'IssueCommentEvent', 7: 'CommitCommentEvent'}\n",
      "\n",
      "A_initial 298.0\n",
      "A_last 1420.0 \n",
      "\n",
      "\n",
      "TEST\n",
      "9082 events between 284 users loaded\n",
      "0 communication events\n",
      "0 assocition events\n"
     ]
    }
   ],
   "source": [
    "train_set = GithubDataset('train', data_dir=data_dir)\n",
    "test_set = GithubDataset('test', data_dir=data_dir)\n",
    "initial_embeddings = np.random.randn(train_set.N_nodes, 32)\n",
    "A_initial = train_set.get_Adjacency()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:48.463520Z",
     "start_time": "2021-09-13T20:29:48.458850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738046"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromtimestamp(time.time(), tz=timezone.utc).toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:48.654177Z",
     "start_time": "2021-09-13T20:29:48.648217Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:50.081058Z",
     "start_time": "2021-09-13T20:29:50.077707Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### some random try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:50.503788Z",
     "start_time": "2021-09-13T20:29:50.499420Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 6, 'PushEvent', datetime.datetime(2013, 1, 1, 3, 53, 4))"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.all_events[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:29:50.762147Z",
     "start_time": "2021-09-13T20:29:50.739555Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t.toordinal() 734865 \n",
      " self.FIRST_DATE.toordinal() 734865 \n",
      "\n",
      " t.toordinal() 734865 \n",
      " self.FIRST_DATE.toordinal() 734865 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-391-38a2abbe5d15>:58: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(247,\n",
       " 17,\n",
       " array([[ 6.,  7., 39., 27.],\n",
       "        [ 6.,  7., 39., 27.]]),\n",
       " 3,\n",
       " array([1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09,\n",
       "        1.3567392e+09, 1.3567392e+09, 1.3567392e+09, 1.3567392e+09]),\n",
       " tensor([1.3573e+09], dtype=torch.float64))"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T18:45:27.452445Z",
     "start_time": "2021-09-13T18:45:27.444586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357289009.0\n",
      "2013-01-04 03:43:29\n"
     ]
    }
   ],
   "source": [
    "# transfer to datetime format\n",
    "print(users_events['juliangruber'][0]['created_at'])\n",
    "print(datetime.fromtimestamp(users_events['juliangruber'][0]['created_at']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T18:45:28.112693Z",
     "start_time": "2021-09-13T18:45:28.105195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': 1357289009.0, 'type': 1, 'login': 'hij1nx'}\n"
     ]
    }
   ],
   "source": [
    "for ind, event in enumerate(users_events['juliangruber']):\n",
    "    print(event)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T18:45:28.415448Z",
     "start_time": "2021-09-13T18:45:28.402280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "check_symmetric(train_set.A_last)## Symmetric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T18:45:28.712531Z",
     "start_time": "2021-09-13T18:45:28.701692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(74, 6, 'PushEvent', datetime.datetime(2013, 1, 1, 3, 53, 4)),\n",
       " (103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 9, 42, 13)),\n",
       " (103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 9, 43, 27)),\n",
       " (103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 9, 45, 54)),\n",
       " (103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 9, 47, 34)),\n",
       " (211, 247, 'WatchEvent', datetime.datetime(2013, 1, 1, 12, 27, 47)),\n",
       " (163, 248, 'WatchEvent', datetime.datetime(2013, 1, 1, 13, 43, 1)),\n",
       " (103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 16, 22, 1)),\n",
       " (74, 6, 'PushEvent', datetime.datetime(2013, 1, 1, 16, 47, 7)),\n",
       " (103, 268, 'CommitCommentEvent', datetime.datetime(2013, 1, 1, 16, 51, 9))]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.all_events[:10] # a list of all event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T20:05:56.331704Z",
     "start_time": "2021-09-13T20:05:56.328270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2013, 1, 1, 3, 53, 4)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.all_events[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T19:50:56.213618Z",
     "start_time": "2021-09-13T19:50:56.209850Z"
    }
   },
   "outputs": [],
   "source": [
    "u, v, rel, time_cur = train_set.all_events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$t_p - \\bar{t_p_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T19:54:10.802019Z",
     "start_time": "2021-09-13T19:54:10.791612Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c,j :  0 74 \n",
      "\n",
      "2012-12-29 00:00:00 2013-01-01 03:53:04\n",
      "td 3 days, 3:53:04 \n",
      "\n",
      "c,j :  1 6 \n",
      "\n",
      "2012-12-29 00:00:00 2013-01-01 03:53:04\n",
      "td 3 days, 3:53:04 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c, j in enumerate([u, v]):\n",
    "    '''\n",
    "    c: index\n",
    "    j: u, v\n",
    "    \n",
    "    '''\n",
    "    print('c,j : ', c, j, '\\n')\n",
    "    t = datetime.fromtimestamp(datetime(2012, 12, 29).timestamp())\n",
    "    print(t, time_cur)\n",
    "    td = time_cur - t\n",
    "    print('td', td, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T04:32:54.302403Z",
     "start_time": "2021-09-14T04:32:54.295939Z"
    }
   },
   "outputs": [],
   "source": [
    "init, keys, last = train_set.get_Adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T04:32:56.628289Z",
     "start_time": "2021-09-14T04:32:56.623590Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T04:33:04.902877Z",
     "start_time": "2021-09-14T04:33:04.893352Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T17:25:14.093598Z",
     "start_time": "2021-09-14T17:25:14.089462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FollowEvent': 0,\n",
       " 'ForkEvent': 1,\n",
       " 'PushEvent': 2,\n",
       " 'WatchEvent': 3,\n",
       " 'IssuesEvent': 4,\n",
       " 'IssueCommentEvent': 5,\n",
       " 'PullRequestEvent': 6,\n",
       " 'CommitCommentEvent': 7}"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.event_types_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary of Github Dataset.\n",
    "\n",
    "###### Graph structure:\n",
    "- node: 284 Github users; static\n",
    "- adjacent: if \"follow\" happened, not symmetric\n",
    "- interaction: all 8 types are seen as the same `communication`\n",
    "\n",
    "###### Train and Test Split\n",
    "\n",
    "they are splited by time\n",
    "- Training: Dec 2012 to Aug 2013, 10000 communications\n",
    "    - both `communication` and `association` events are used for training\n",
    "- Test: Sep 2013 to Dec 2013, 8000 communications\n",
    "    - give tuple $(u, v, \\tau, k)$ (the positive sample) compute the conditional density of $u$ with all other nodes and rank them (MAR). See how top $v$ ranks.\n",
    "        - $u$: node; $\\tau$: time point of event; $k$: event type\n",
    "       \n",
    "    - Evaluation:\n",
    "        - only on `communication` events, \n",
    "            - cannot learn `association` events well since its sparsity.\n",
    "\n",
    "###### How data looks like?\n",
    "\n",
    "see `train_set.all_events`, a list of tuples = $(u, v, k, t)$.\n",
    "\n",
    "###### How data is loaded into model?\n",
    "\n",
    "define 2 things:\n",
    "1. `EventsDataset(torch.utils.data.Dataset)`. \n",
    "    1. the object meet the requirement of `torch.utils.data.Dataset`\n",
    "\n",
    "2. `GithubDataset(EventsDataset)`. Only has 2 components:\n",
    "    1. some special attributes\n",
    "    2. get_adjacency method\n",
    "    \n",
    "    \n",
    "    \n",
    "###### How could model train (mini-batch)\n",
    "\n",
    "- a batch is a randomly selected interactions\n",
    "- each epoch is a \"full training process\" \n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "Overall structure of data and data loading:\n",
    "\n",
    "\n",
    "\n",
    "in `data_loader.py`:\n",
    "\n",
    "---\n",
    "\n",
    "1. define a child of `torch.utils.data.Dataset`:\n",
    "\n",
    "```python\n",
    "class EventsDataset(torch.utils.data.Dataset):\n",
    "    def __init__():\n",
    "        # nothing here (all attributes move to the `MADataset` child)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_events # return the attribute of child-child \n",
    "    def __geitem__(self, index):\n",
    "        '''\n",
    "        returns:\n",
    "        - time_delta_uv:\n",
    "        \n",
    "        '''\n",
    "        return u, v, time_delta_uv, k, time_bar, time_cur\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**What are the returns [u, v, time_delta_uv, k, time_bar, time_cur] ?**\n",
    "\n",
    "- `time_cur`: datetime timestamp instance; a time for event happened\n",
    "    - e.g. run `train_set.all_events[0][3]`\n",
    "- `td`: time difference btw `time_cur` and `t`\n",
    "    - `t` = datetime timestamp; time of last event happened for u(or v)\n",
    "        - `t` is extracted from `time_bar`: stores the most recent (event happened time) for all nodes (a np array with length 284); each element is a datetime timestamp instance.\n",
    "        \n",
    "\n",
    "- `time_delta_uv`: a 2x4 np array, \n",
    "    - the 0th row store u's time difference `td`;\n",
    "    - the 1st row ...\n",
    "\n",
    "- `time_bar`: stores the most recent (event happened time) for all nodes (a np array with length 284); each element is a datetime timestamp instance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "2. define a child of `EventsDataset`:\n",
    "\n",
    "\n",
    "```python\n",
    "class MADataset(EventsDataset):\n",
    "    super() # could inherite all method and attribute from the parent\n",
    "     \n",
    "    def __init__():\n",
    "        \n",
    "        # a lot of attributes ....\n",
    "        self.n_events\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_Adjacency():\n",
    "        return Adj_all_start, keys, Adj_all_last\n",
    "    \n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "in `main.py`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyrep:\n",
    "\n",
    "```python\n",
    "class Dyrep()\n",
    "\n",
    "    def __init__():\n",
    "    \n",
    "    \n",
    "    def forward():\n",
    "\n",
    "        return lambda_uv_t, lambda_uv_t_non_events / self.N_surv_samples, [A_pred, Surv], S_batch, reg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
